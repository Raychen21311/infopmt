# -*- coding: utf-8 -*-
"""
sys.py â€” RFP/å¥‘ç´„ å¯©æŸ¥ï¼ˆè³‡è¨Šè™•æª¢æ ¸ç‰ˆï¼‰ + é å…ˆå¯©æŸ¥è¡¨ï¼ˆPDF å°ˆç”¨ï¼‰

åŠŸèƒ½ï¼š
- ä¸Šå‚³ RFP/å¥‘ç´„ PDFï¼ˆå¯è¤‡é¸ï¼‰â†’ ä¾æª¢æ ¸æ¸…å–®æª¢æ ¸ï¼ˆä¸€æ¬¡æ€§/æ‰¹æ¬¡/é€é¡Œï¼‰
- ä¸Šå‚³ã€ŒåŸ·è¡Œå–®ä½é å…ˆå¯©æŸ¥è¡¨ã€PDFï¼ˆå¯è¤‡é¸/å¯ç•¥éï¼‰â†’ LLM çµæ§‹åŒ–æŠ½å–
- å…ˆé¡¯ç¤ºã€é å¯©è¾¨è­˜è¡¨ã€‘ï¼ˆå›ºå®š 5 æ¬„ï¼šç·¨è™Ÿ/æª¢æ ¸é …ç›®/é å¯©"evidence":åˆ¤å®š/"å°æ‡‰é æ¬¡/å‚™è¨»"ï¼‰
- ç”¢ç”Ÿã€å·®ç•°å°ç…§è¡¨ã€‘ï¼ˆé å¯© vs. ç³»çµ±æª¢æ ¸ï¼‰ï¼Œæ”¯æ´åªé¡¯ç¤ºä¸ä¸€è‡´/ç¼ºæ¼
- åŒ¯å‡º Excelï¼ˆä¸‰å€‹å·¥ä½œè¡¨ï¼‰ï¼šæª¢æ ¸çµæœ / é å¯©è¾¨è­˜ / å·®ç•°å°ç…§

è¦æ ¼é‡é»ï¼š
- é å¯©åˆ¤å®šåƒ…å…è¨±ï¼šç¬¦åˆ / ä¸é©ç”¨ï¼›æœªå‹¾é¸é¡¯ç¤ºç‚ºç©ºç™½ï¼ˆèƒŒæ™¯æ¯”å°æ­£è¦åŒ–ç‚ºã€ŒæœªæåŠã€ï¼‰
- A0ï¼ˆæ¡ˆä»¶æ€§è³ªï¼‰ç‚ºå…­é¸ä¸€å­—é¢å€¼ï¼Œæ¯”å°æ™‚æ¡å­—é¢å€¼æ¯”å°ï¼Œä¸èµ°å››æ…‹
- ç·¨è™Ÿæ¨™æº–åŒ–ï¼ˆcompute_std_idï¼‰ï¼šä¸­æ–‡ç« ç¯€â†’ä»£ç¢¼ï¼ˆA..Fï¼‰+ æ•¸å­—ï¼ˆå«å°æ•¸ï¼‰ï¼›ã€Œå…¶ä»–é‡é»ã€â†’ F
- æª¢æ ¸æ¸…å–®åŒ…å« F é¡ï¼›æ‰¹æ¬¡å¯©æŸ¥åˆ†çµ„ç‚ºï¼šABï½œCDEF
"""

import os, re, json, io
from typing import List, Dict, Any, Tuple
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode
import streamlit as st
import fitz  # PyMuPDF
import pandas as pd
from dotenv import load_dotenv
import google.generativeai as genai
from difflib import SequenceMatcher

# -------------------- LLM --------------------
load_dotenv()
if os.getenv('GOOGLE_API_KEY'):
    genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
model = genai.GenerativeModel("gemini-2.5-flash")

# -------------------- æª”æ¡ˆå‹æ…‹ --------------------
def is_pdf(name: str) -> bool:
    return name.lower().endswith(".pdf")

# ==================== æª¢æ ¸æ¸…å–®ï¼ˆå« F å…¶ä»–é‡é»ï¼‰ ====================
def build_rfp_checklist() -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    def add(cat, code, text): items.append({"category":cat, "id":code, "item":text})

    # A åŸºæœ¬èˆ‡å‰æ¡ˆ
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A0",   "æœ¬æ¡ˆå±¬é–‹ç™¼å»ºç½®ã€ç³»çµ±ç¶­é‹ã€åŠŸèƒ½å¢ä¿®ã€å¥—è£è»Ÿé«”ã€ç¡¬é«”ã€å…¶ä»–?")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A1",   "æœ¬æ¡ˆç‚ºå»¶çºŒæ€§åˆç´„ï¼Œå‰æ¡ˆæ¡è³¼ç°½é™³å½±æœ¬å·²é™„ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.1", "æœ¬æ¡ˆäº‹å‰æ›¾èˆ‡è³‡è¨Šè™•è¨è«–ï¼šæœ¬æ¡ˆç›¸é—œæŠ€è¡“æ–‡ä»¶ç”±è³‡è¨Šè™•å”åŠ©æ’°å¯«ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.2", "æœ¬æ¡ˆäº‹å‰æ›¾èˆ‡è³‡è¨Šè™•è¨è«–ï¼šè¦åŠƒéšæ®µæ›¾èˆ‡è³‡è¨Šè™•é–‹æœƒè¨è«–æ¡è³¼å…§å®¹ä¸¦æœ‰æœƒè­°ç´€éŒ„ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.3", "æœ¬æ¡ˆç°½è¾¦ä¹‹å‰ï¼Œå·²ä»¥è«‹è¾¦å–®éäº¤å¥‘ç´„æ›¸ã€éœ€æ±‚èªªæ˜æ›¸ç­‰ç›¸é—œæ–‡ä»¶ï¼Œé€è«‹è³‡è¨Šè™•æª¢è¦–ï¼Œä¸¦ä¿ç•™è‡³å°‘5å€‹å·¥ä½œå¤©ä¹‹å¯©é–±æœŸå¾Œå–å¾—å›è¦†ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.4", "æœ¬æ¡ˆäº‹å‰æœªèˆ‡è³‡è¨Šè™•è¨è«–ï¼ˆç„¡ï¼‰ã€‚")

    # B ç¾æ³èªªæ˜
    add("B ç¾æ³èªªæ˜", "B1.1", "æä¾›æœ€æ–°ç‰ˆç¡¬é«”è¨­å‚™åŠç¶²è·¯ä¹‹æ¶æ§‹åœ–(ä¸å«IP Address)ï¼šæ˜ç¢ºè¡¨é”ç¡¬é«”æ”¾ç½®å€åŸŸï¼ˆå«æ©Ÿæˆ¿/å€åŸŸï¼‰ã€‚")
    add("B ç¾æ³èªªæ˜", "B1.2", "æä¾›ç¶²è·¯ä»‹æ¥æ–¹å¼èˆ‡é–‹ç™¼å·¥å…·ä¹‹å» ç‰Œã€å‹è™Ÿã€ç‰ˆæœ¬ç­‰è³‡è¨Šã€‚")
    add("B ç¾æ³èªªæ˜", "B2",   "ç½®æ–¼æœ¬éƒ¨æ©Ÿæˆ¿ä¹‹ç³»çµ±ï¼Œå¦‚å¦è¨­å°å¤–é€£ç·šç·šè·¯è€…ï¼Œæä¾›é€£ç·šå°è±¡ã€ç¨®é¡åŠè¦æ ¼æ¸…å–®ã€‚")
    add("B ç¾æ³èªªæ˜", "B3",   "æä¾›ä½¿ç”¨è€…æˆ–ä½¿ç”¨æ©Ÿé—œä¹‹ç¤ºæ„åœ–æˆ–èªªæ˜ã€‚")
    add("B ç¾æ³èªªæ˜", "B4",   "æä¾›æœ€æ–°ç¶²ç«™ç¶²å€ã€‚")
    add("B ç¾æ³èªªæ˜", "B5",   "æä¾›æ‡‰ç”¨ç³»çµ±åŠŸèƒ½æ¸…å–®æˆ–æ¶æ§‹åœ–ï¼ˆå« OSã€DB åç¨±èˆ‡ç‰ˆæœ¬ï¼‰ã€‚")

    # C è³‡å®‰éœ€æ±‚
    add("C è³‡å®‰éœ€æ±‚", "C1.1", "ç¬¦åˆæœ¬éƒ¨æ¡è³¼å¥‘ç´„è¦ç¯„ä¹‹è³‡è¨Šå®‰å…¨èˆ‡å€‹è³‡ä¿è­·è¦æ±‚ï¼šå·²å¡«åˆ—å®‰å…¨ç­‰ç´šä¸”èˆ‡æœ€æ–°æ ¸å®šç­‰ç´šç›¸ç¬¦ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.2", "è¦æ±‚ç³»çµ±ç¬¦åˆã€Šè³‡é€šå®‰å…¨è²¬ä»»ç­‰ç´šåˆ†ç´šè¾¦æ³•ã€‹ä¹‹ã€è³‡é€šç³»çµ±é˜²è­·åŸºæº–ã€ã€SSDLC å„éšæ®µå®‰å…¨å·¥ä½œï¼›è¦æ±‚å» å•†æäº¤ã€è³‡é€šç³»çµ±é˜²è­·åŸºæº–è‡ªè©•è¡¨ã€ä¸¦å¢åˆ—ç½°å‰‡ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.3", "è¦æ±‚å» å•†ä¹‹æœå‹™æ°´æº–æ»¿è¶³ç³»çµ±æœ€å¤§å¯å®¹å¿ä¸­æ–·æ™‚é–“ï¼ˆRTOï¼‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.4", "éç½®æ–¼æœ¬éƒ¨æ©Ÿæˆ¿ä¹‹æ ¸å¿ƒè³‡é€šç³»çµ±ï¼Œç´å…¥ SOC ç¯„åœã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.5", "å§”å¤–éœ€æ±‚æ¶‰åŠè³‡é€šæŠ€è¡“æœå‹™ï¼ˆå¦‚é›²ç«¯ï¼‰å·²è©•ä¼°åˆæ³•æ€§ã€æŠ€è¡“ç¶­é‹ã€æ³•éµèˆ‡æ¬Šåˆ©ç¾©å‹™æ­¸å±¬ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.6", "è¦æ±‚å» å•†ä¸å¾—ä½¿ç”¨æˆ–è¨­è¨ˆä¸ç¬¦å®‰å…¨è¦ç¯„ä¹‹å¸³è™Ÿå¯†ç¢¼ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C2.1", "å·¨é¡/è³‡å®‰æ¡è³¼æˆ–é«˜ç´šå®‰å…¨ç­‰ç´šæ¡ˆä»¶ï¼šæŠ•æ¨™å» å•†å…·å‚™å®‰å…¨è»Ÿé«”é–‹ç™¼èƒ½åŠ›ä¸¦é€šéè³‡å®‰ç®¡ç†ç³»çµ±é©—è­‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C2.2", "å·¨é¡/è³‡å®‰/é«˜ç´šï¼šå°ˆæ¡ˆç®¡ç†äººå“¡è‡³å°‘1äººå…·è³‡è¨Šå®‰å…¨å°ˆæ¥­èªè­‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C2.3", "å·¨é¡/è³‡å®‰/é«˜ç´šï¼šå°ˆæ¡ˆæŠ€è¡“äººå“¡è‡³å°‘1äººå…·ç¶²è·¯å®‰å…¨æŠ€èƒ½ä¹‹è¨“ç·´è­‰æ›¸æˆ–è­‰ç…§ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C3.1", "å…è¨±åˆ†åŒ…è€…ï¼šåˆ†åŒ…å» å•†é ˆæ¯”ç…§æ‰¿åŒ…å» å•†å…±åŒéµå®ˆè³‡å®‰è¦å®šã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C3.2", "å…è¨±åˆ†åŒ…è€…ï¼šæŠ•æ¨™å» å•†æ–¼æœå‹™å»ºè­°æ›¸æ•˜æ˜åˆ†åŒ…å» å•†åŸºæœ¬è³‡æ–™ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C4",   "ä¸å¾—æ¡ç”¨å¤§é™¸å» ç‰Œè³‡é€šè¨Šç”¢å“ï¼ˆå¥‘ç´„è‰æ¡ˆç¬¬å…«æ¢(å…­)åŠ(äºŒ äº”)ï¼‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C5",   "ç¬¦åˆã€è³‡é€šç³»çµ±ç±Œç²å„éšæ®µè³‡å®‰å¼·åŒ–æªæ–½åŸ·è¡Œæª¢æ ¸è¡¨ã€ï¼ˆé–‹ç™¼é™„è¡¨1/ç¶­é‹é™„è¡¨2ï¼‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C6",   "è³‡æ–™åº«ä¸­æ©Ÿæ•è³‡æ–™å·²æ¡ç”¨æˆ–è¦åŠƒé©ç•¶åŠ å¯†æŠ€è¡“ã€‚")

    # D ä½œæ¥­éœ€æ±‚ï¼ˆç¯€éŒ„ï¼‰
    add("D ä½œæ¥­éœ€æ±‚", "D1",  "åˆ—å‡ºæ‰€éœ€è»Ÿç¡¬é«”èˆ‡ç¶²è·¯è¨­å‚™æ¸…å–®ï¼Œèªªæ˜ä½¿ç”¨è³‡è¨Šè™•è¨­å‚™/æ—¢æœ‰è¨­å‚™æˆ–å¦è¡Œæ¡è³¼ï¼ˆå„ªå…ˆ VM/å…±åŒä¾›æ‡‰å¥‘ç´„ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D2",  "ç³»çµ±é–‹ç™¼æˆ–åŠŸèƒ½å¢ä¿®æ‡‰åˆ—å‡ºæ‰€éœ€ç³»çµ±åŠŸèƒ½ï¼ˆåœ°æ–¹æ”¿åºœç³»çµ±å»ºè­°æä¾›è³‡æ–™ä¸‹è¼‰æˆ–ä»‹æ¥ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D3",  "æ•˜æ˜è³‡è¨Šç³»çµ±èˆ‡å…¶ä»–è»Ÿé«”ç³»çµ±ä¹‹ç›¸äº’é—œä¿‚ä¸¦èªªæ˜æ‰€æœ‰åˆ©å®³é—œä¿‚äººã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D4",  "æä¾›æ°‘çœ¾ä¸‹è¼‰æª”æ¡ˆè€…ï¼Œå¢åŠ  ODF æ ¼å¼ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D5",  "é–‹ç™¼ App å·²é–±è®€ä¸¦éµå¾ªåœ‹ç™¼æœƒç›¸é—œè¦å®šï¼ˆé™„ä»¶2ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D6",  "é–‹ç™¼ App ç¬¦åˆé€šå‚³æœƒã€App ç„¡éšœç¤™é–‹ç™¼æŒ‡å¼•ã€ä¸¦å¡«å ±æª¢æ ¸è¡¨ï¼ˆé™„ä»¶3ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D7",  "ç¶²ç«™æœå‹™ä¹‹ç³»çµ±ç¬¦åˆåœ‹ç™¼æœƒã€æ”¿åºœç¶²ç«™æœå‹™ç®¡ç†è¦ç¯„ã€ä¸¦å¡«å ±æª¢æ ¸è¡¨ï¼ˆé™„ä»¶4ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D8",  "é‡å°æ¥­å‹™æˆ–å€‹äººè³‡æ–™ï¼Œæä¾›å¾ŒçºŒ OpenData æˆ– MyData æœå‹™å»ºè­°ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D9",  "ç³»çµ±ç¶­è­·åŒ…å«å®šæœŸåˆ°å ´ã€ç·Šæ€¥åˆ°å ´ã€è«®è©¢æœå‹™ï¼›SLA èˆ‡ç¸¾æ•ˆæŒ‡æ¨™é€£å‹•ä¸¦è¨­è¨ˆä½¿ç”¨è€…æ»¿æ„åº¦èª¿æŸ¥ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D10", "å±¥ç´„æœå‹™éŠœæ¥å¥‘ç´„æœŸé–“ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D11", "é–‹ç™¼åŠæ¸¬è©¦è¨­å‚™èˆ‡ç’°å¢ƒéœ€æ±‚èªªæ˜ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D12", "æ•™è‚²è¨“ç·´åŠå®¢æœæœå‹™ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D13", "ä¿å›ºæœå‹™ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D14", "ç”¢å“æˆæ¬Š (License) ç¬¦åˆéœ€æ±‚ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D15", "ä½œæ¥­éœ€æ±‚å¿…é ˆç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼ˆè©³è¨»5ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D16", "å¦‚æœ‰ GIS / OpenData / MyData ä½œæ¥­éœ€æ±‚ï¼Œç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼ˆè©³è¨»6ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D17", "ä¸Šç·šå‰å®Œæˆéœ€æ±‚è¨ªè«‡ã€éœ€æ±‚ç¢ºèªèˆ‡æ¸¬è©¦ï¼ˆå«æ•ˆèƒ½æ¸¬è©¦ï¼‰ï¼›æäº¤æ¸¬è©¦è¨ˆç•«èˆ‡æ¸¬è©¦å ±å‘Šã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D18", "æ¶‰åŠé†«ç™‚/å¥åº·è³‡æ–™äº¤æ›è€…ï¼Œç´å…¥ FHIR äº¤æ›æ¨™æº–ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D19", "åŠŸèƒ½éœ€æ±‚è¨­è¨ˆè€ƒé‡å°å…¥ AI ä»¥ç¯€çœäººåŠ›/é¿å…éŒ¯èª¤èˆ‡æ±ºç­–åˆ†æåŠé¢¨éšªé è­¦ã€‚")

    # E ç”¢å“äº¤ä»˜
    add("E ç”¢å“äº¤ä»˜", "E1", "äº¤ä»˜æ™‚ç¨‹åˆç†ï¼Œä¸¦èˆ‡é–‹ç™¼æ–¹å¼ï¼ˆç€‘å¸ƒ/æ•æ·ï¼‰ä¸€è‡´ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E2", "é–‹ç™¼/å¢ä¿®äº¤ä»˜å“å®Œæ•´ï¼ˆå°ˆæ¡ˆè¨ˆç•«ã€éœ€æ±‚/è¨­è¨ˆã€æ¸¬è©¦è¨ˆç•«/å ±å‘Šã€å»ºç½®è¨ˆç•«ã€æ‰‹å†Šã€æ•™è‚²è¨“ç·´ã€ä¿å›ºç´€éŒ„ã€åŸå§‹ç¢¼/åŸ·è¡Œç¢¼ã€æœ€é«˜æ¬Šé™å¸³å¯†ã€è‡ªè©•è¡¨èˆ‡é›»å­æª”ï¼‰ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E3", "ç¶­è­·äº¤ä»˜å“ï¼ˆå°ˆæ¡ˆåŸ·è¡Œè¨ˆç•«ã€ç¶­è­·å·¥ä½œå ±å‘Šã€æœ€æ–°ç‰ˆè¨­è¨ˆ/æ‰‹å†Šã€æœ€æ–°ç‰ˆåŸå§‹ç¢¼/åŸ·è¡Œç¢¼ã€è‡ªè©•è¡¨èˆ‡é›»å­æª”ï¼‰ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E4", "å¿…é ˆç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼ˆè©³è¨»8ï¼‰ï¼šäº¤ä»˜ä¹‹åŸå§‹ç¨‹å¼ç¢¼ã€åŸ·è¡Œç¢¼ï¼Œæœ¬éƒ¨å¾—è¦æ±‚æ‰¿åŒ…å» å•†æ–¼æœ¬éƒ¨æŒ‡å®šä¹‹ç’°å¢ƒé€²è¡Œå†ç”Ÿæ¸¬è©¦ï¼Œä¸¦æ‡‰æä¾›æ‰€ä½¿ç”¨ä¹‹é–‹ç™¼å·¥å…·ï¼Œä»¥é©—è­‰å…¶æ­£ç¢ºæ€§ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E5", "ç¶²è·¯è¨­å‚™è³¼ç½®æ™‚ï¼Œé©—æ”¶ä»¥å½Œå°æ–¹å¼äº¤ä»˜å¸³å¯†ã€è¨­å®šæª”ã€è¦å‰‡åˆ—è¡¨èˆ‡æ¶æ§‹ç­‰ã€‚")

    # F å…¶ä»–é‡é»ï¼ˆä¾é å¯©è¡¨ï¼‰
    add("F å…¶ä»–é‡é»", "F1", "ä½¿ç”¨æœ¬éƒ¨æ©Ÿæˆ¿ VMã€å…±ç”¨è³‡æ–™åº«ï¼Œå·²å®Œæˆè©•ä¼°åŠæˆæœ¬åˆ†æ”¤è¡¨å¡«å¯«ï¼Œä¸¦å·²åˆ†æ”¤ç¶“è²»ã€‚")
    add("F å…¶ä»–é‡é»", "F2", "ç¶“è²»é ä¼°ä¹‹åˆç†æ€§åŠç¶“è³‡é–€æ­¸é¡ä¹‹æ­£ç¢ºæ€§ï¼›ç¶­è­·è²»ç”¨è¨ˆç®—æ¯”ç‡æ‡‰é€å¹´éæ¸›ï¼ˆä¸å«æ—¢æœ‰æ“´å¢ä¸”éä¿å›ºéƒ¨åˆ†ï¼‰ã€‚")
    add("F å…¶ä»–é‡é»", "F3", "æ¡è³¼å…§å®¹ç„¡å‰å¾Œä¸ä¸€è‡´æƒ…å½¢ã€‚")
    add("F å…¶ä»–é‡é»", "F4", "å°ç…§ä½œæ¥­éœ€æ±‚æª¢æŸ¥å¥‘ç´„æ›¸ã€Œæœå‹™æ°´æº–åŠç¸¾æ•ˆé•ç´„é‡‘ã€ä¹‹å…§å®¹æœ‰ç„¡ç¼ºæ¼ã€‚")
    add("F å…¶ä»–é‡é»", "F5", "æ¡è³¼å¥‘ç´„æ›¸ã€Œå±¥ç´„æ¨™çš„ã€å…§å®¹æ­£ç¢ºï¼Œç„¡ç¼ºæ¼ã€‚")
    add("F å…¶ä»–é‡é»", "F6", "é–‹ç™¼æˆ–å¢ä¿®ç³»çµ±ä¹‹ä»‹æ¥å…§å®¹ï¼Œå·²æ´½ç›¸é—œå–®ä½åŒæ„ï¼Œä¸¦ç¢ºèªå°æ–¹ç³»çµ±å¢ä¿®åŠç¶“è²»ä¾†æºã€‚")
    add("F å…¶ä»–é‡é»", "F7", "ç›®å‰ä½¿ç”¨ä¹‹ç¡¬é«”è¨­å‚™æ–¼å±¥ç´„å®Œæˆå¾Œï¼Œå¦‚æ±°æ›æˆ–ä¸å†ä½¿ç”¨è€…ï¼Œè¦åŠƒä¸‹æ¶æ—¥æœŸã€‚")
    add("F å…¶ä»–é‡é»", "F8", "æº–ç”¨æœ€æœ‰åˆ©æ¨™ä¹‹è©•é¸é …ç›®èˆ‡é…åˆ†ï¼Œé™„éŒ„å¯ä¾éœ€æ±‚èª¿æ•´æ•´ä½µä¸¦åŒæ­¥ä¿®æ­£ç›¸é—œé™„éŒ„ã€‚")

    return items

# ==================== åˆ†ç¾¤/æ’åºå·¥å…·ï¼ˆæ‰¹æ¬¡æ”¹ç‚º ABï½œCDEFï¼‰ ====================
def group_items_by_ABCDE(items: List[Dict[str, Any]]) -> List[Tuple[str, List[Dict[str, Any]]]]:
    return [("ABCDE", items)] if items else []

def group_items_by_AB_CDE(items: List[Dict[str, Any]]) -> List[Tuple[str, List[Dict[str, Any]]]]:
    """ä¿ç•™å‡½å¼åä»¥ç›¸å®¹ï¼Œä½†ç¬¬äºŒçµ„å·²æ“´å……ç‚º CDEFã€‚"""
    ab   = [it for it in items if it['id'] and it['id'][0] in ('A','B')]
    cdef = [it for it in items if it['id'] and it['id'][0] in ('C','D','E','F')]
    groups = []
    if ab:   groups.append(('AB', ab))
    if cdef: groups.append(('CDEF', cdef))
    return groups

# é€é¡Œæ’åºï¼ˆAâ†’Bâ†’Câ†’Dâ†’Eâ†’Fï¼‰
def order_items_AB_C_D_E(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    order_map = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5}
    return sorted(items, key=lambda it: (order_map.get(it['id'][0], 9), it['id']))

# ==================== PDF è§£æ ====================
def extract_text_with_headers(pdf_bytes: bytes, filename: str) -> str:
    doc = fitz.open(stream=pdf_bytes, filetype='pdf')
    parts = []
    for i, page in enumerate(doc, start=1):
        text = page.get_text('text').strip()
        if not text:
            blocks = page.get_text('blocks')
            text = "\n\n".join([b[4].strip() for b in blocks if b[4].strip()])
        parts.append(f"\n\n===== ã€æª”æ¡ˆ: {filename} é : {i}ã€‘ =====\n" + text)
    return "\n".join(parts)

# ==================== LLM Prompts ====================

def make_batch_prompt(batch_code: str, items: List[Dict[str, Any]], corpus_text: str) -> str:
    checklist_lines = "\n".join([f"{it['id']}ï½œ{it['item']}" for it in items])

    batch_prompt = """
ä½ æ˜¯æ”¿åºœæ©Ÿé—œè³‡è¨Šè™•ä¹‹æ¡è³¼/RFP/å¥‘ç´„å¯©æŸ¥å§”å“¡ã€‚è«‹ä¾ä¸‹åˆ—ã€Œæª¢æ ¸æ¢ç›®ï¼ˆ{batch_code} æ‰¹ï¼‰ã€é€æ¢å¯©æŸ¥æ–‡ä»¶å…§å®¹ä¸¦å›å‚³**å”¯ä¸€ JSON é™£åˆ—**ï¼Œé™£åˆ—å…§æ¯å€‹å…ƒç´ å°æ‡‰ä¸€æ¢æ¢ç›®ã€‚
ã€å¯©æŸ¥åŸå‰‡ã€‘
1) åƒ…ä¾æ–‡ä»¶æ˜è¼‰å…§å®¹åˆ¤æ–·ã€‚
2) è‹¥å±¬ä¸é©ç”¨ï¼ˆä¾‹ï¼šæœªå…è¨±åˆ†åŒ…ï¼‰ï¼Œè«‹å›ã€Œä¸é©ç”¨ã€ä¸¦èªªæ˜ä¾æ“šã€‚
3) å‹™å¿…å¼•ç”¨åŸæ–‡çŸ­å¥èˆ‡æª”å/é ç¢¼ä½œç‚º evidenceã€‚
4) ***åš´ç¦è¼¸å‡ºä»»ä½•èˆ‡è¦æ ¼è¯çµ¡äººã€é›»è©±ã€å§“åã€è¯ç¹«æ–¹å¼æœ‰é—œçš„æ–‡å­—ï¼Œå³ä½¿åŸå§‹æ–‡ä»¶å…§æœ‰ã€‚***
5) å¯©æŸ¥æ™‚æ‡‰åƒé–±é å¯©ç›¸é—œèªªæ˜ï¼Œé¿å…æœªåƒè€ƒé å¯©èªªæ˜è€Œèª¤åˆ¤ã€‚
6) è‹¥ id = 'A1'ï¼Œè«‹å›å¾©"è«‹æª¢è¦–æ˜¯å¦å·²é™„å‰æ¡ˆæ¡è³¼ç°½é™³å½±æœ¬ï¼Œä»¥ç¢ºä¿æ¡è³¼æµç¨‹çš„å»¶çºŒæ€§èˆ‡åˆæ³•æ€§æª¢è¦–åŸºç¤ã€‚"
7) è‹¥ id = 'A2.1ã€A2.2ã€A2.3'ï¼Œè‹¥é å¯©è¡¨ç‚ºä¸é©ç”¨ï¼Œè«‹å›å¾©"èˆ‡è³‡è¨Šè™•ä¹‹ç›¸é—œå¯¦å‹™æƒ…å½¢ï¼Œè«‹æ‰¿è¾¦äººå†æ¬¡æ ¸å¯¦ã€‚"
9) è‹¥ id = 'B1'ï¼Œç¬¦åˆæƒ…å½¢ï¼šæœ‰åŒ…å«è¨­å‚™ã€ç¶²è·¯ã€æ©Ÿæˆ¿æ¶æ§‹åœ–ã€æœ‰å¯«ç¡¬é«”æ”¾ç½®å€åŸŸç‚º"è¡›ç”Ÿç¦åˆ©éƒ¨å¤–"æˆ–"è¡›ç”Ÿç¦åˆ©éƒ¨å…§ã€‚
10) è‹¥ id = 'B2'ï¼Œç¬¦åˆæƒ…å½¢ï¼šæœ‰åŒ…å«ç¶²è·¯æ¶æ§‹åœ–ï¼Œä¸€èˆ¬åŸå‰‡ä¸Šå‡ç„¡å°å¤–é€£ç·šä¹‹ç¶²è·¯ï¼Œæ¶æ§‹åœ–æ¥åªç•«å‡ºå°å…§éƒ¨ç·šè·¯åœ–ã€‚

12) è‹¥ id = 'C3.1ã€C3.2'ï¼ŒåŸå‰‡ä¸Šéœ€æ±‚æ›¸æˆ–å¥‘ç´„æ›¸ç„¡æåŠ"æŠ•æ¨™å» å•†æœ‰éœ€å†åˆ†åŒ…çµ¦å…¶ä»–å» å•†ç­‰ç›¸é—œéœ€æ±‚"ï¼Œå³ç‚º"ä¸é©ç”¨"ï¼Œè‹¥æœ‰éœ€è©³ç›¡èªªæ˜ã€‚
13) è‹¥ id = 'D4'ï¼Œå¦‚ç„¡å…¬é–‹çµ¦æ°‘çœ¾ä½¿ç”¨ä¹‹ç³»çµ±æ‡‰ç‚ºä¸é©ç”¨ã€è‹¥æœ‰æå‡ºODFç›¸é—œéœ€æ±‚äº¦å¯ç‚ºç¬¦åˆï¼Œä½†é ˆèªªæ˜ä¸€ä¸‹ã€‚
14) è‹¥ id = 'D5ã€D6'ï¼Œç„¡Appéœ€æ±‚å…§å®¹å³ç‚ºä¸é©ç”¨ã€‚
15) è‹¥ id = 'D16'ï¼Œç„¡ GISã€OpenDataã€MyData ä½œæ¥­éœ€æ±‚ï¼Œæ‡‰ç‚ºä¸é©ç”¨ã€‚
16) è‹¥ id = 'D5ã€D6'ï¼Œç¬¦åˆå…©æ¢ä»¶ä¸­ä¹‹ä¸€ä»¶å³å¯ã€‚
17) è‹¥ id = 'D15'ï¼Œè¨»5.ä½œæ¥­éœ€æ±‚å¿…é ˆç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼š
*æœ¬å°ˆæ¡ˆæ‰€ä½¿ç”¨ä¹‹ç³»çµ±è»Ÿé«”(å¦‚ä½œæ¥­ç³»çµ±ã€è³‡æ–™åº«è»Ÿé«”ç­‰)æˆ–ç€è¦½å™¨è»Ÿé«”ç‰ˆæœ¬æ›´æ–°æ™‚ï¼Œæ–¼æœ¬éƒ¨é€š
çŸ¥é™æœŸå…§ï¼Œå» å•†é ˆå°æ‡‰ç”¨ç³»çµ±é€²è¡Œä¿®æ­£ã€‚
*æ”¯æ´ç³»çµ±å‚™æ´åŠç·Šæ€¥å›å¾©å·¥ä½œï¼Œä¸¦è¦–æœ¬éƒ¨è¦æ±‚é…åˆåŸ·è¡Œç³»çµ±é·ç§»ä¸åŒä¼ºæœä¸»æ©Ÿä½œæ¥­ã€‚
*ç¶­æŒæœ¬å°ˆæ¡ˆç³»çµ±èˆ‡å…¶ä»–ç›¸é—œç³»çµ±æ•´åˆä»‹é¢ä¹‹æ­£å¸¸é‹ä½œï¼ˆå«ä»‹æ¥ä½œæ¥­æ‰€éœ€ä¹‹æ¬„ä½å¢ä¿®ï¼‰ã€‚
*æœ¬å°ˆæ¡ˆç³»çµ±å»ºç½®éç¨‹é ˆé©ç•¶å°å…¥è™›æ“¬åŒ–æŠ€è¡“ä¸¦é ç•™æ—¥å¾Œæ“´å……ä¹‹å¯èƒ½æ€§ï¼Œä»¥æ¸›å°‘å„ç³»çµ±é‡è¤‡å»º
ç½®è³‡æºä¹‹æˆæœ¬ã€‚
*æä¾›æœ¬å°ˆæ¡ˆç³»çµ±ä½¿ç”¨è€…å¸³è™Ÿæ¸…æŸ¥ç´€éŒ„è¡¨ã€‚
*æä¾›æœ¬å°ˆæ¡ˆç³»çµ±ä½¿ç”¨è€…å¸³è™Ÿç™»å‡ºã€å…¥ã€ç•°å¸¸ç´€éŒ„æŸ¥è©¢åˆ—è¡¨ã€‚
*æœ¬å°ˆæ¡ˆç³»çµ±ä¹‹è³‡æ–™ç•°å‹•ï¼Œéœ€è¨˜éŒ„ç•°å‹•å…§å®¹ã€æ“ä½œè€…ã€æ™‚é–“ã€IP ç­‰è³‡è¨Šï¼Œä»¥ä¾›å¾ŒçºŒè¿½è¹¤æŸ¥è©¢ã€‚
(å…§å«é‡è¦è³‡æ–™ä¹‹ç³»çµ±æ™‚è«‹åˆ—å…¥)
*æœ¬å°ˆæ¡ˆè‹¥å±¬æ–°å»ºç½®ç³»çµ±æ–¼è³‡æ–™åº«è¦æ ¼è¨­è¨ˆæ™‚é ˆå¼•ç”¨æ”¿åºœè³‡æ–™æ¨™æº–å¹³è‡º
(https://schema.gov.tw/)æ‰€å…¬å¸ƒç›¸é—œé ˜åŸŸè³‡æ–™æ¨™æº–ä¹‹è³‡æ–™æ¬„ä½å‘½ååŠæ ¼å¼å®šç¾©ï¼›è‹¥å±¬èˆŠæœ‰
ç³»çµ±ä¸¦è² æœ‰OPENDATAã€MYDATA æˆ–äº¤æ›è³‡æ–™çš„æœå‹™æ™‚ï¼Œæ‡‰é€æ­¥å°‡è³‡æ–™åº«çš„ç›¸é—œæ¬„ä½åƒç…§å‰è¿°ç›¸
é—œé ˜åŸŸè³‡æ–™æ¨™æº–é€²è¡Œæ ¼å¼æ¨™æº–åŒ–ï¼Œä»¥æ¸›å°‘è³‡æ–™é‹ç”¨å–®ä½çš„è³‡æ–™æ¸…ç†(ETL)æ™‚é–“ï¼Œå¼·åŒ–ç³»çµ±æœå‹™
çš„è³‡æ–™å³æ™‚æ€§ã€‚
18) è‹¥ id = 'D16'ï¼Œè¨»6.GISã€OPENDATAã€MYDATA ä½œæ¥­éœ€æ±‚å¿…é ˆç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼š
(a)GISï¼ˆåœ°ç†åœ–è³‡ç³»çµ±ï¼‰ï¼š
*ç©ºé–“è³‡è¨Šé ˆæ¡ç”¨å…¨åœ‹é€šç”¨ä¹‹åæ¨™ç³»çµ±ï¼ˆTWD97ã€WGS84 EPSG:3857ã€WGS84 EPSG:4326ï¼‰ã€‚
*è¼¸å…¥è³‡æ–™æ ¼å¼é ˆè‡³å°‘ç‚ºCSVã€XLSã€XLSXã€XMLã€JSONã€GeoJSONã€KMLã€KMZã€æˆ–SHPï¼Œè¼¸å‡º
è³‡æ–™æ ¼å¼è‡³å°‘ç‚ºCSVã€GeoJSONã€KMLã€KMZã€æˆ–SHPã€‚è¼¸å…¥åŠè¼¸å‡ºä¹‹è³‡æ–™å…§å®¹ä¸¦é ˆç¬¦åˆåœ‹ç™¼
æœƒã€Œæ”¿åºœè³‡æ–™å“è³ªæå‡æ©Ÿåˆ¶é‹ä½œæŒ‡å¼•ã€ä¹‹è¦ç¯„ã€‚
*è¼¸å‡ºè³‡æ–™é ˆæœ‰å›ºå®šä¸‹è¼‰é€£çµï¼Œæˆ–æä¾›ç¬¦åˆã€Œåœ‹ç™¼æœƒå…±é€šæ€§æ‡‰ç”¨ç¨‹å¼ä»‹é¢è¦ç¯„ã€ä¹‹ä»‹æ¥æ–¹å¼ã€‚
*å»ºç½®åœ°åœ–æœå‹™å¦‚æœ‰ä½¿ç”¨å„å¼ä¸»é¡Œåœ–è³‡ã€å®šä½æŸ¥è©¢ã€åŠåˆ†æåŠŸèƒ½ä¹‹éœ€æ±‚ï¼Œæ‡‰å„ªå…ˆä½¿ç”¨å…§æ”¿éƒ¨
TGOS æä¾›ä¹‹å„å¼æœå‹™è³‡æºï¼ˆhttps://www.tgos.twï¼‰ï¼›å¦ä½¿ç”¨ä¹‹åº•åœ–ï¼Œæ‡‰ä½¿ç”¨å…§æ”¿éƒ¨åœ‹åœŸæ¸¬
ç¹ªä¸­å¿ƒæä¾›ä¹‹è‡ºç£é€šç”¨é›»å­åœ°åœ–å„å¼åº•åœ–è³‡æºï¼ˆhttps://maps.nlsc.gov.tw/ï¼‰ã€‚
*åŒ¯å…¥ç³»çµ±ä¹‹è³‡æ–™å…§å®¹å¦‚åŒ…æ‹¬åœ°å€ï¼Œå‰‡é ˆè‡ªå‹•è½‰ç‚ºåæ¨™ã€‚
*ä½¿ç”¨ä»˜è²»ç¶²è·¯è³‡æºå»ºç½®ç³»çµ±æˆ–åœ–å°ï¼Œè³¼ç½®æˆæ¬Šæ•¸é‡é ˆç¬¦åˆé ä¼°ç”¨é‡ã€‚
*ä½¿ç”¨ä»˜è²»è»Ÿé«”å»ºç½®GIS ä¼ºæœå™¨ï¼Œè³¼è²·æˆæ¬Šæ•¸é‡é ˆç¬¦åˆé ä¼°è¨ˆç®—èˆ‡ç™¼ä½ˆèƒ½é‡éœ€æ±‚ã€‚
(b)OPENDATAï¼ˆæ”¿åºœè³‡æ–™é–‹æ”¾ï¼‰ï¼š
*è³‡æ–™(åº«)æ¬„ä½è¨­è¨ˆé ˆå¼•ç”¨æ”¿åºœè³‡æ–™æ¨™æº–å¹³è‡º(https://schema.gov.tw/)æ‰€å…¬å¸ƒç›¸é—œé ˜åŸŸ
è³‡æ–™æ¨™æº–ä¹‹è³‡æ–™æ¬„ä½å‘½ååŠæ ¼å¼å®šç¾©ã€‚
*è¼¸å…¥è³‡æ–™æ ¼å¼éœ€è‡³å°‘ç‚ºCSVã€XLSã€XLSXã€XML æˆ–JSONï¼Œè¼¸å‡ºè³‡æ–™æ ¼å¼è‡³å°‘ç‚ºCSVã€XML æˆ–
JSONã€‚è¼¸å…¥åŠè¼¸å‡ºä¹‹è³‡æ–™å…§å®¹ä¸¦é ˆç¬¦åˆåœ‹ç™¼æœƒã€Œæ”¿åºœè³‡æ–™å“è³ªæå‡æ©Ÿåˆ¶é‹ä½œæŒ‡å¼•ã€ä¹‹è¦ç¯„ã€‚
(c)MYDATAï¼ˆæ•¸ä½æœå‹™å€‹äººåŒ–ï¼‰ï¼š
*æ‡‰ä¾æ“šåœ‹å®¶ç™¼å±•å§”å“¡æœƒã€Œæ•¸ä½æœå‹™å€‹äººåŒ–å¹³è‡ºä»‹æ¥ä½œæ¥­è©¦è¾¦è¦é»ã€è¾¦ç†ï¼Œæä¾›æ°‘çœ¾ä¸‹è¼‰åŠ
é‹ç”¨å…¶å€‹äººåŒ–è³‡æ–™ï¼Œä¸¦ä¿éšœè³‡è¨Šå®‰å…¨åŠå€‹äººéš±ç§æ¬Šç›Šã€‚
*å…·è³‡æ–™æä¾›è€…è§’è‰²ä¹‹ç³»çµ±ï¼Œé ˆæ–¼æ•¸ä½æœå‹™å€‹äººåŒ–(MyData)ç®¡ç†å¾Œè‡ºç™»å…¥å¡«å¯«è³‡æ–™å‚³è¼¸é …ç›®
åŠé–‹ç™¼ä»‹æ¥ç¨‹å¼ï¼Œç¶“ç•¶äº‹äººå®Œæˆèº«åˆ†é©—è­‰åŠåŒæ„å¾Œï¼Œå‚³è¼¸æä¾›è©²å€‹äººåŒ–è³‡æ–™ã€‚
*å…·è³‡æ–™æä¾›è€…è§’è‰²ä¹‹ç³»çµ±ï¼Œæ‡‰åƒç…§åœ‹å®¶ç™¼å±•å§”å“¡æœƒç¯„ä¾‹è£½ä½œåŠä¸Šå‚³ã€Œæ•¸ä½æœå‹™å€‹äººåŒ–
(MyData)ä»‹æ¥è³‡æ–™æª”æ¡ˆè¦æ ¼æ›¸ã€ã€‚
*å…·æœå‹™æä¾›è€…è§’è‰²ä¹‹ç³»çµ±ï¼Œé ˆæ–¼æ•¸ä½æœå‹™å€‹äººåŒ–(MyData)ç®¡ç†å¾Œè‡ºç™»å…¥å¡«å¯«æœå‹™é …ç›®ï¼ŒåŠ
åƒç…§å·²é–‹æ”¾ä¹‹æ•¸ä½æœå‹™å€‹äººåŒ–(MyData)ä»‹æ¥è³‡æ–™æª”æ¡ˆè¦æ ¼æ›¸é–‹ç™¼ä»‹æ¥ç¨‹å¼ï¼Œç¶“ç•¶äº‹äººå®Œæˆ
èº«åˆ†é©—è­‰åŠåŒæ„å¾Œï¼Œå–å¾—è©²ç•¶äº‹äººå€‹äººåŒ–è³‡æ–™ä¸¦è™•ç†é‹ç”¨ã€‚
*å…·æœå‹™æä¾›è€…è§’è‰²ä¹‹ç³»çµ±ï¼Œæ‡‰æ–¼ç·šä¸Šæœå‹™æ­ç¤ºæœå‹™æ¢æ¬¾ï¼ŒåŒ…å«æœå‹™åç¨±ã€æœå‹™ç›®çš„èˆ‡å…§å®¹ã€
è³‡æ–™é›†é …ç›®ã€è³‡æ–™å„²å­˜èˆ‡ä½¿ç”¨æ–¹å¼ã€ç•¶äº‹äººç›¸é—œæ¬Šåˆ©ç­‰å…§å®¹ã€‚
*æ‡‰ç”¢è£½ç•¶äº‹äººè³‡æ–™å‚³è¼¸ç›¸é—œç´€éŒ„ä¸¦ä¿å­˜è‡³å°‘1 å¹´ï¼Œç´€éŒ„å…§å®¹è‡³å°‘åŒ…å«å‚³è¼¸æ™‚é–“ã€å‚³è¼¸å°è±¡ã€
ç•¶äº‹äººèº«åˆ†ã€è³‡æ–™å‚³è¼¸æˆåŠŸèˆ‡å¦ç­‰ã€‚
*é–‹ç™¼æ•¸ä½æœå‹™å€‹äººåŒ–(MyData)å¹³è‡ºä»‹æ¥ç¨‹å¼ï¼Œè«‹åƒè€ƒã€Œæ•¸ä½æœå‹™å€‹äººåŒ–(MyData)æ‡‰ç”¨è¦ç¯„ã€
åŠå…¶æŠ€è¡“æ–‡ä»¶(https://github.com/ehousekeeper/emsg)ã€‚ã€‚
19)è‹¥ id = 'D17'ï¼Œå¦‚éç³»çµ±ç§»è½‰æˆ–ä¸Šç·šï¼Œæ‡‰ç‚ºä¸é©ç”¨ã€‚
20)è‹¥ id = 'D19'ï¼Œå¦‚ç„¡å°å…¥ AI æŠ€è¡“ä¹‹ç›¸é—œéœ€æ±‚ï¼Œæ‡‰ç‚ºä¸é©ç”¨ã€‚
21)è‹¥ id = 'E1'ï¼Œæ–‡ä»¶æœ‰ç´€éŒ„äº¤ä»˜æ™‚ç¨‹å³ç¬¦åˆï¼Œå¦‚é•å»ºç½®æ¡ˆé ˆæ³¨æ„é–‹ç™¼æ–¹å¼ç­‰ç›¸é—œå…§å®¹ã€‚
22)è‹¥ id = 'F7'ï¼Œæ–‡ä»¶æœ‰ç´€éŒ„äº¤ä»˜æ™‚ç¨‹å³ç¬¦åˆï¼Œå¦‚å¦‚æ–‡ä»¶æœªæåŠå‰‡æ‡‰ç‚ºä¸é©ç”¨ã€‚
23) è‹¥ id = 'A0'ï¼Œä»¥é å¯©è¡¨åˆ¤å®šç‚ºä¸»(ä»¥æ¥­å‹™å–®ä½éœ€æ±‚ç‚ºä¸»)ï¼Œå¯å¤šé¸ã€‚
24)è«‹æ³¨æ„***æª¢æ ¸é …ç›®éœ€å®Œå…¨èˆ‡æª¢æ ¸æ¢ç›®ä¸€æ¨¡ä¸€æ¨£***ã€‚
ã€è¼¸å‡ºæ ¼å¼ â€” åƒ…èƒ½è¼¸å‡º JSON é™£åˆ—ï¼Œç„¡ä»»ä½•å¤šé¤˜æ–‡å­—/æ¨™è¨˜ã€‘
[
  {{
    "id": "A1",
    "category": "A åŸºæœ¬èˆ‡å‰æ¡ˆ",
    "item": "æ¢ç›®åŸæ–‡ï¼ˆè«‹å®Œæ•´è¤‡è£½ï¼‰",
    "compliance": "è‹¥ id = 'A0'ï¼šåƒ…èƒ½è¼¸å‡ºå…­é¸ä¸€ã€é–‹ç™¼å»ºç½®ï½œç³»çµ±ç¶­é‹ï½œåŠŸèƒ½å¢ä¿®ï½œå¥—è£è»Ÿé«”ï½œç¡¬é«”ï½œå…¶ä»–ã€‘ï¼›è‹¥ id â‰  'A0'ï¼šåƒ…èƒ½è¼¸å‡ºå››é¸ä¸€ã€ç¬¦åˆï½œéƒ¨åˆ†ç¬¦åˆï½œæœªæåŠï½œä¸é©ç”¨ã€‘ï¼›ç¦æ­¢åŒæ™‚è¼¸å‡ºå¤šå€‹æˆ–å…¶ä»–æ–‡å­—",
    "evidence": [{{"file": "æª”å", "page": é ç¢¼, "quote": "é€å­—å¼•è¿°"}}],
    "recommendation": "è‹¥æœªæåŠ/éƒ¨åˆ†ç¬¦åˆï¼Œè«‹çµ¦å…·é«”è£œå¼·æ–¹å‘ï¼›å¦å‰‡ç•™ç©º"
  }}
]
ã€æœ¬æ‰¹æª¢æ ¸æ¸…å–®ï¼ˆidï½œitemï¼‰ã€‘
{checklist_lines}
ã€æ–‡ä»¶å…¨æ–‡ï¼ˆå«æª”å/é ç¢¼æ¨™è¨»ï¼‰ã€‘
{corpus_text}
""".strip()
    return batch_prompt

def make_single_prompt(item: Dict[str, Any], corpus_text: str) -> str:
    return make_batch_prompt(item['id'], [item], corpus_text)

# ï¼ˆé å¯©è¡¨æŠ½å–ï¼šé å¯©åˆ¤å®šåƒ…å…è¨±ã€ç¬¦åˆ/ä¸é©ç”¨ã€‘ï¼Œæœªå‹¾é¸è¼¸å‡ºç©ºå­—ä¸²ï¼›A0 ä¾‹å¤–ç‚ºå…­é¸ä¸€å­—é¢ï¼‰
def make_precheck_parse_prompt(corpus_text: str) -> str:
    return f"""
ä½ æ˜¯æ”¿åºœæ©Ÿé—œè³‡è¨Šè™•ä¹‹æ¡è³¼å¯©æŸ¥åŠ©ç†ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½æˆ–å¤šä»½ã€ŒåŸ·è¡Œå–®ä½é å…ˆå¯©æŸ¥è¡¨ã€çš„ PDF æ–‡å­—ï¼ˆå·²æ¨™è¨»æª”åèˆ‡é ç¢¼ï¼‰ã€‚
è«‹å°‡è¡¨æ ¼/æ¢åˆ—é€åˆ—è½‰ç‚º **JSON é™£åˆ—**ï¼Œæ¯åˆ—ä¸€ç­†ï¼Œæ¬„ä½å¦‚ä¸‹ï¼ˆé¡¯ç¤ºåƒ…ç”¨åˆ°å‰äº”æ¬„ï¼Œå…¶é¤˜åƒ…ä¾›åˆ¤æ–·ç”¨ï¼‰ï¼š

ã€é¡¯ç¤ºç”¨å¿…å¡« 5 æ¬„ã€‘
- "id": å…ˆå¡«ä½ èƒ½è¾¨è­˜çš„ç²—ç·¨è™Ÿï¼ˆå¦‚ã€Œæ¡ˆä»¶æ€§è³ª-1.ã€ã€Œç¾æ³èªªæ˜-1.(2)ã€ã€ŒA2.3ã€ç­‰ï¼›è‹¥ç„¡å¯ç•™ç©ºï¼‰
- "item": æª¢æ ¸é …ç›®ï¼ˆä¸è¦çœç•¥ï¼‰
- "status": åƒ…èƒ½è¼¸å‡ºäºŒé¸ä¸€ã€ç¬¦åˆï½œä¸é©ç”¨ã€‘ï¼›è‹¥è©²åˆ—æœªå‹¾é¸ä»»ä½•é¸é …ï¼Œè«‹è¼¸å‡ºç©ºå­—ä¸² ""
- "biz_ref_note": å°æ‡‰é æ¬¡æˆ–è£œå……èªªæ˜

ã€è¼”åŠ©åˆ¤æ–·æ¬„ï¼ˆå¯ç¼ºæ¼ï¼‰ã€‘
- "section_title": ç« ç¯€æ¨™é¡Œï¼ˆå¦‚ã€Œæ¡ˆä»¶æ€§è³ªã€ã€Œç¾æ³èªªæ˜ã€ã€Œè³‡å®‰éœ€æ±‚ã€ã€Œä½œæ¥­éœ€æ±‚ã€ã€Œç”¢å“äº¤ä»˜ã€ã€Œå…¶ä»–é‡é»ã€ï¼‰
- "main_no": ä¸»è™Ÿï¼ˆå¦‚ 1, 2, 3ï¼‰
- "sub_no": æ¬¡è™Ÿï¼ˆå¦‚ 1, 2ï¼›è‹¥ç„¡å¯çœç•¥ï¼‰
- "std_id": è‹¥èƒ½ä¾ä¸‹æ–¹è¦å‰‡ç›´æ¥è¨ˆç®—å‡ºç³»çµ±æ¨™æº– IDï¼ˆA..F + æ•¸å­—[.æ•¸å­—]ï¼‰ï¼Œè«‹å¡«ï¼›å¦å‰‡ç©ºå­—ä¸²
- "evidence": æ¯åˆ—è‡³å°‘ä¸€ç­†ï¼š{{"file": æª”å, "page": é ç¢¼, "quote": å¼•è¿°çŸ­å¥}}

ã€é‡è¦ç‰ˆé¢è¦å‰‡ï¼ˆè«‹åš´æ ¼éµå¾ªï¼‰ã€‘
1) æœ¬è¡¨ã€Œè¡¨é ­ã€é€šå¸¸ç‚ºï¼šã€Œæª¢æ ¸å…§å®¹ï½œç¬¦åˆï½œä¸é©ç”¨ï½œå°æ‡‰é æ¬¡/å‚™è¨»ã€ã€‚
2) å‹¾é¸ç¬¦è™Ÿæ¡ **â– =å·²å‹¾ã€â–¡=æœªå‹¾**ã€‚
3) ç•¶çœ‹åˆ°å­é … (1)(2)(3)â€¦ ä¹‹å¾Œç·Šæ¥è‘—å…©åˆ—æˆ–å¤šåˆ—å…¨æ˜¯ã€Œâ– /â–¡ã€çš„çŸ©é™£æ™‚ï¼š
   - **ç¬¬ä¸€åˆ—**çš„æ¯ä¸€æ ¼ä¾åºå°æ‡‰å­é … (1)(2)(3)â€¦ çš„ **ã€Œç¬¦åˆã€** æ¬„çµæœï¼›
   - **ç¬¬äºŒåˆ—**çš„æ¯ä¸€æ ¼ä¾åºå°æ‡‰å­é … (1)(2)(3)â€¦ çš„ **ã€Œä¸é©ç”¨ã€** æ¬„çµæœï¼›
   - å°‡æ¯å€‹å­é …æ‹†æˆç¨ç«‹åˆ—ï¼ˆä¾‹å¦‚ã€ŒA2.1ã€ã€ŒA2.2ã€ã€ŒA2.3ã€â€¦ï¼‰ï¼Œä¸¦ä¾è©²å­é …åœ¨çŸ©é™£åŒåºä½æ ¼å­çš„ã€Œâ– /â–¡ã€æ±ºå®š "status"ã€‚
   - ä¾‹å¦‚ï¼šè‹¥å››å€‹å­é …å¾Œé¢å‡ºç¾ç¬¬ä¸€åˆ—ï¼š`â–¡ â–¡ â–¡ â–¡`ã€ç¬¬äºŒåˆ—ï¼š`â–  â–  â–  â– `ï¼Œå‰‡å››å€‹å­é …å‡ç‚º **"ä¸é©ç”¨"**ã€‚
4) è‹¥ç„¡çŸ©é™£ã€è€Œæ˜¯æ¯åˆ—æ–‡å­—å³å´å„è‡ªå‡ºç¾ã€Œç¬¦åˆ/ä¸é©ç”¨ã€å‹¾é¸ï¼Œè«‹å°±è¿‘åˆ¤æ–·è©²åˆ—çš„ "status"ã€‚
5) **ä¸å¾—çŒœæ¸¬**ï¼šè‹¥ç¢ºå¯¦æ²’æœ‰ä»»ä½•ã€Œç¬¦åˆ/ä¸é©ç”¨ã€çš„å‹¾é¸è·¡è±¡ï¼Œ"status" è«‹å›ç©ºå­—ä¸² ""ï¼Œä¸¦æä¾› evidenceã€‚

ã€A0 ç‰¹ä¾‹ï¼ˆå…­é¸ä¸€ï¼‰ã€‘
- è‹¥æª¢å‡ºã€Œæ¡ˆä»¶æ€§è³ªã€é¡å‹å‹¾é¸ï¼ˆé–‹ç™¼å»ºç½®/ç³»çµ±ç¶­é‹/åŠŸèƒ½å¢ä¿®/å¥—è£è»Ÿé«”/ç¡¬é«”/å…¶ä»–ï¼‰ï¼Œè«‹é¡å¤–æ–°å¢ä¸€åˆ— A0ï¼š
  {{
    "id": "A0", "item": "æ¡ˆä»¶æ€§è³ªï¼ˆå…­é¸ä¸€ï¼‰",
    "status": "ï¼ˆå¡«è¢«å‹¾é¸çš„é¡å‹å­—æ¨£ï¼‰",   # A0 ç‚ºå­—é¢å€¼ï¼Œéã€Œç¬¦åˆ/ä¸é©ç”¨ã€
    "biz_ref_note": "",
    "section_title": "æ¡ˆä»¶æ€§è³ª", "main_no": 0, "std_id": "A0",
    "evidence": [{{"file":"...", "page": é ç¢¼, "quote":"..."}}]
  }}

ã€å®‰å…¨è¦ç¯„ã€‘
- åƒ…ä¾æ–‡ä»¶æ˜è¼‰å…§å®¹ï¼›ä¸å¯ç™¼æ˜ã€‚
- **ç¦æ­¢è¼¸å‡ºä»»ä½•è¯çµ¡è³‡è¨Šï¼ˆå§“åã€é›»è©±ã€Email ç­‰ï¼‰ï¼Œå³ä½¿æ–‡ä»¶å…§æœ‰ã€‚**

ã€è¼¸å‡ºæ ¼å¼ â€” åƒ…èƒ½è¼¸å‡º JSON é™£åˆ—ï¼Œç„¡å¤šé¤˜æ–‡å­—ã€‘
[
  {{
    "id": "ç¾æ³èªªæ˜-1.(2)",
    "item": "é€éä½•ç¨®ç¶²è·¯æ¶æ§‹â€¦ä¸¦èªªæ˜å» ç‰Œã€å‹è™Ÿã€ç‰ˆæœ¬ç­‰ã€‚",
    "status": "ç¬¦åˆ",        # æˆ– "ä¸é©ç”¨"ï¼›è‹¥æœªå‹¾é¸å‰‡è¼¸å‡º ""
    "biz_ref_note": "éœ€æ±‚èªªæ˜æ›¸ P.11-12ç­‰æ–‡å­—",
    "section_title": "ç¾æ³èªªæ˜",
    "main_no": 1,
    "sub_no": 2,
    "std_id": "B1.2",
    "evidence": [{{"file":"xxx.pdf","page":2,"quote":"â€¦"}}]
  }}
]

ã€æ–‡ä»¶å…¨æ–‡ï¼ˆå«æª”å/é ç¢¼æ¨™è¨»ï¼‰ã€‘
{corpus_text}
""".strip()
def make_reply_prompt(corpus_text: str) -> str:
    reply_ad = """
è«‹ä¾ç…§ä½¿ç”¨è€…ä¸Šå‚³æ–‡å­—ç”Ÿæˆå»ºè­°å›å¾©ï¼Œå›å¾©é ˆåŒ…å«å››é»ï¼š
ä¸€ï¼šã€Œæœ¬æ¡ˆæ¡è³¼é‡‘é¡å¤šå°‘è¬å…ƒï¼ŒåŒ…å«ç³»çµ±ç¶­é‹ã€åŠŸèƒ½å¢ä¿®ç­‰ã€‚ã€(æ›´æ–°é‡‘é¡å¤šå°‘è¬å…ƒå³å¯ï¼Œä¾‹å¦‚1,000è¬å…ƒ)
äºŒï¼šã€Œè³‡è¨Šç³»çµ±ä¹‹ç¶­é‹è²»ç”¨æ‡‰é€å¹´éæ¸›ï¼Œå» å•†å ±åƒ¹å¦‚æœ‰å¢é•·ï¼Œå¯è«‹å» å•†æ–¼æœ¬æ¡ˆä¹‹æœŸæœ«å ±å‘Šæä¾›ç³»çµ±ä½¿ç”¨æ•ˆç›ŠæŒ‡æ¨™ï¼Œåšç‚ºæ¬¡å¹´ç¶­é‹è²»ç”¨æˆé•·ä¹‹åˆ¤æ–·ã€‚ã€(è«‹å‹¿ä¿®æ”¹ï¼Œè«‹å‹¿è£œå……ï¼Œä¸€å­—ä¸æ¼ç›´æ¥å›å¾©)
ä¸‰ã€åˆ—å‡ºé¡å¤–åƒè€ƒé¸é …
ã€Œæœ‰é—œé†«é™¢è³‡æ–™æ²»ç†å·¥ä½œå…§å®¹ï¼Œè«‹åƒé–±æœ¬éƒ¨é†«ç™‚è³‡è¨Šå¤§å¹³å°ä¹‹é†«ç™‚è³‡è¨Šæ¨™æº–ï¼Œå¦‚FHIRã€LOINCã€SNOMED CTã€RxNormï¼Œä¸”ç¬¦åˆä¸‰å¤§AIä¸­å¿ƒã€SMART on FHIRç­‰ä½œæ¥­äº‹é …ã€‚å¦å¦‚æœ‰TWCDIåŠIGéœ€æ±‚ï¼Œå¯è‡³è©²å¹³å°ææ¡ˆã€‚ã€
ã€Œæœ‰é—œæª¢æ ¸è¡¨ï¼Œå·²è«‹å–®ä½æ‰¿è¾¦äººé…Œä¿®å®Œç•¢åˆ¶å¼æ–‡å¥æª¢æ ¸å…§å®¹ã€‚ã€
""".strip()

    return f"""
ä½ æ˜¯æ”¿åºœæ©Ÿé—œè³‡è¨Šè™•çš„æ¡è³¼å¯©æŸ¥åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å…§å®¹æ’°å¯«ä¸€æ®µæ­£å¼çš„å»ºè­°å›å¾©æ–‡å­—ã€‚
{reply_ad}

ã€æ–‡ä»¶å…¨æ–‡ã€‘
{corpus_text}
""".strip()

# ==================== è§£æ/è½‰è¡¨å·¥å…· ====================
def parse_json_array(text: str) -> List[Dict[str, Any]]:
    t = text.strip()
    # å»é™¤å¯èƒ½çš„ ```json / ``` åŒ…è£¹
    t = re.sub(r'^```(?:json)?', '', t, flags=re.I).strip()
    t = re.sub(r'```$', '', t, flags=re.I).strip()
    if t.startswith('{') and t.endswith('}'):
        try:
            d = json.loads(t); return [d]
        except Exception:
            pass
    start = t.find('['); end = t.rfind(']')
    if start != -1 and end != -1 and end > start:
        t = t[start:end+1]
    data = json.loads(t)
    if isinstance(data, dict):
        data = [data]
    return data

def _format_evidence_list(e_list: List[Dict[str, Any]]) -> str:
    lines = []
    for e in e_list:
        file = e.get('file','')
        page = e.get('page', None)
        quote = e.get('quote','')
        tag = f"p.{page}" if page not in (None, "", "n/a") else ""
        lines.append(f"{file} {tag}ï¼š{quote}".strip())
    return "\n".join(lines)

def normalize_status_equiv(s: str) -> str:
    """
    é å¯©ç«¯åƒ…å…©æ…‹ï¼ˆç¬¦åˆ/ä¸é©ç”¨ï¼‰ï¼Œå…¶é¤˜/ç©ºç™½ä¸€å¾‹è¦–ç‚ºã€ŒæœªæåŠã€ä»¥åˆ©èˆ‡ç³»çµ±å››æ…‹æ¯”å°ã€‚
    """
    if s is None:
        return "æœªæåŠ"
    t = re.sub(r"\s+", "", str(s)).lower()
    if t == "":
        return "æœªæåŠ"
    if t in ("ç¬¦åˆ", "ok", "pass", "é€šé"):
        return "ç¬¦åˆ"
    if t in ("ä¸é©ç”¨", "na", "n/a"):
        return "ä¸é©ç”¨"
    # ä»»ä½•å…¶å®ƒå­—çœ¼ï¼ˆå¦‚ä¸ç¬¦åˆ/éœ€è£œä»¶/æ”¹å–„ç­‰ï¼‰ä¸€å¾‹è¦–ç‚ºã€ŒæœªæåŠã€ï¼ˆå› é å¯©æ­£å¼åªç”¨å…©æ…‹ï¼‰
    return "æœªæåŠ"

# ç« ç¯€ â†’ ä»£è™Ÿï¼ˆå« Fï¼‰
SECTION_TO_LETTER = {
    "æ¡ˆä»¶æ€§è³ª": "A",
    "ç¾æ³èªªæ˜": "B",
    "è³‡å®‰éœ€æ±‚": "C",
    "ä½œæ¥­éœ€æ±‚": "D",
    "ç”¢å“äº¤ä»˜": "E",
    "å…¶ä»–é‡é»": "F",
}
ROMAN_TO_LETTER = {
    "ä¸€": "A", "äºŒ": "B", "ä¸‰": "C", "å››": "D", "äº”": "E", "å…­": "F"
}
STD_ID_PATTERN = re.compile(r"^[A-F]\d+(?:\.\d+)?$")

def compute_std_id(raw_id: str, item: str) -> str:
    """
    å°‡é å¯©åˆ—çš„åŸå§‹ç·¨è™Ÿ/æ–‡å­—ï¼Œè½‰æ›æˆç³»çµ±æ¨™æº– IDï¼ˆA..F + æ•¸å­—[.æ•¸å­—]ï¼‰ã€‚
    è‹¥ç„¡æ³•å¯é åˆ¤å®šï¼Œå›å‚³ç©ºå­—ä¸²ï¼ˆåœ¨å°ç…§è¡¨æœƒè½ç‚ºã€é å¯©å¤šå‡ºã€ï¼‰ã€‚
    """
    s = (raw_id or "").strip()
    # å·²æ˜¯æ¨™æº– ID
    if STD_ID_PATTERN.match(s):
        return s

    # å¾å…§å®¹æ¨æ–·ç« ç¯€
    src = f"{raw_id} {item}".strip()
    sec_letter = ""
    for zh, letter in SECTION_TO_LETTER.items():
        if zh in src:
            sec_letter = letter
            break
    if not sec_letter:
        for zh, letter in ROMAN_TO_LETTER.items():
            if f"{zh}ã€" in src or f"{zh} " in src:
                sec_letter = letter
                break

    # æŠ“ä¸»è™Ÿï¼ˆå„ªå…ˆåŒ¹é… "X-1.(2)" ä¹‹ç¬¬ä¸€å€‹æ•¸å­—ï¼›å¦å‰‡æŠ“æ–‡ä¸­ç¬¬ä¸€å€‹æ•¸å­—ï¼‰
    m_main = re.search(r"-\s*(\d+)", raw_id or "") or re.search(r"(\d+)\s*[ã€\.ï¼)]", src)
    n1 = m_main.group(1) if m_main else None

    # æŠ“æ¬¡è™Ÿ "(1)" "(2)"
    m_sub = re.search(r"\((\d+)\)", src)
    n2 = m_sub.group(1) if m_sub else None

    if sec_letter and n1:
        return f"{sec_letter}{n1}" + (f".{n2}" if n2 else "")
    return ""

# è§£æé å¯© JSON â†’ è£½ä½œ 5 æ¬„é¡¯ç¤ºè¡¨ï¼Œä¸¦ä¿ç•™éš±è—æ¬„ä½ä¾›æ¯”å°/é™¤éŒ¯
def parse_precheck_json(text: str) -> List[Dict[str, Any]]:
    data = parse_json_array(text)
    rows = []
    for r in data if isinstance(data, list) else []:
        if not isinstance(r, dict):
            continue
        ev = []
        for e in r.get("evidence", []):
            if not isinstance(e, dict): 
                continue
            ev.append({
                "file": e.get("file",""),
                "page": e.get("page", None),
                "quote": e.get("quote","")
            })
        rows.append({
            "raw_id": r.get("id","").strip(),                 # LLM ç²—ç·¨è™Ÿï¼ˆåŸæ¨£ï¼‰
            "item": r.get("item","").strip(),
            "status": r.get("status","").strip(),             # åƒ…å…è¨±ï¼šç¬¦åˆ/ä¸é©ç”¨/""(æœªå‹¾)
            "biz_ref_note": r.get("biz_ref_note","").strip(),
            "section_title": r.get("section_title","").strip(),
            "main_no": r.get("main_no", None),
            "sub_no": r.get("sub_no", None),
            "std_id": r.get("std_id","").strip(),             # è‹¥æ¨¡å‹å·²ç®—å‡º
            "evidence": ev,                                    # ä¿ç•™ä½†ä¸é¡¯ç¤º
        })
    return rows

def precheck_rows_to_df(rows: List[Dict[str, Any]]) -> pd.DataFrame:
    # å…ˆæ±‚å‡ºæ¨™æº– IDï¼ˆè‹¥ LLM æ²’çµ¦ std_idï¼Œå°±ç”¨ compute_std_id æ¨æ–·ï¼‰
    std_ids = []
    for r in rows:
        sid = r.get("std_id","")
        if not sid:
            sid = compute_std_id(r.get("raw_id",""), r.get("item",""))
        std_ids.append(sid)

    # é¡¯ç¤ºå°ˆç”¨ 5 æ¬„ï¼ˆã€Œé å¯©åˆ¤å®šã€ç‚ºåŸå­—ï¼šç¬¦åˆ/ä¸é©ç”¨/ç©ºç™½ï¼‰
    df = pd.DataFrame({
        "ç·¨è™Ÿ":     std_ids,                         # ä½¿ç”¨æ¨™æº– IDï¼›ç©ºç™½ä»£è¡¨ç„¡å°æ‡‰
        "æª¢æ ¸é …ç›®": [r.get("item","") for r in rows],
        "é å¯©åˆ¤å®š": [r.get("status","") for r in rows],
        "å°æ‡‰é æ¬¡/å‚™è¨»": [r.get("biz_ref_note","") for r in rows],  # 

    })

    # èƒŒæ™¯éš±è—æ¬„ä½ï¼ˆä¾›å·®ç•°å°ç…§èˆ‡é™¤éŒ¯ï¼‰â€”â€”ä¸é¡¯ç¤ºã€ä¸åŒ¯å‡º
    df["_é å¯©ç­‰åƒ¹ç´š_éš±è—"] = df["é å¯©åˆ¤å®š"].apply(normalize_status_equiv)  # ç¬¦åˆ/ä¸é©ç”¨/æœªæåŠ
    df["_raw_id_éš±è—"] = [r.get("raw_id","") for r in rows]
    df["_section_éš±è—"] = [r.get("section_title","") for r in rows]
    return df

# ==================== ç³»çµ±æª¢æ ¸ â†’ DataFrameï¼ˆæ’åºå« Fï¼‰ ====================
def to_dataframe(results: List[Dict[str, Any]]) -> pd.DataFrame:
    rows = []
    for r in results:
        ev_text = "\n".join([f"{e.get('file','')} p.{e.get('page','')}ï¼š{e.get('quote','')}" for e in r.get('evidence', [])])
        rows.append({
            "é¡åˆ¥": r.get("category",""),
            "ç·¨è™Ÿ": r.get("id",""),
            "æª¢æ ¸é …ç›®": r.get("item",""),
            "ç¬¦åˆæƒ…å½¢": r.get("compliance",""),
            "ä¸»è¦è­‰æ“š": ev_text,
            "æ”¹å–„å»ºè­°": r.get("recommendation",""),
        })
    df = pd.DataFrame(rows)
    # å‹å–„æ’åºï¼ˆAâ†’Bâ†’Câ†’Dâ†’Eâ†’Fï¼‰
    try:
        df["ä¸»ç¢¼"] = df["ç·¨è™Ÿ"].str.extract(r"([A-F])")
        df["å­ç¢¼å€¼"] = pd.to_numeric(df["ç·¨è™Ÿ"].str.extract(r"(\d+(?:\.\d+)?)")[0], errors='coerce')
        code_order = {"A":0,"B":1,"C":2,"D":3,"E":4,"F":5}
        df["ä¸»åº"] = df["ä¸»ç¢¼"].map(code_order).fillna(9)
        df = df.sort_values(["ä¸»åº","å­ç¢¼å€¼","ç·¨è™Ÿ"], kind='mergesort').drop(columns=["ä¸»ç¢¼","å­ç¢¼å€¼","ä¸»åº"])
    except Exception:
        pass
    return df

# ==================== é å¯© vs ç³»çµ± æª¢æ ¸ï¼šå·®ç•°å°ç…§ï¼ˆå·²ä¿®æ­£ KeyErrorï¼‰ ====================
def fuzzy_match(best_of: List[str], query: str) -> Tuple[str, float]:
    best_id, best_ratio = "", 0.0
    for cand in best_of:
        r = SequenceMatcher(a=query, b=cand).ratio()
        if r > best_ratio:
            best_ratio, best_id = r, cand
    return best_id, best_ratio

def build_compare_table(sys_df: pd.DataFrame, pre_df: pd.DataFrame) -> pd.DataFrame:
    """
    sys_df ä¾†è‡ª to_dataframe(): æ¬„ä½ [é¡åˆ¥, ç·¨è™Ÿ, æª¢æ ¸é …ç›®, ç¬¦åˆæƒ…å½¢, ä¸»è¦è­‰æ“š, æ”¹å–„å»ºè­°]
    pre_df ä¾†è‡ªé å¯©è¾¨è­˜ï¼š      æ¬„ä½ [ç·¨è™Ÿ, æª¢æ ¸é …ç›®, é å¯©åˆ¤å®š, å°æ‡‰é æ¬¡/å‚™è¨», _é å¯©ç­‰åƒ¹ç´š_éš±è—]
    """
    # é—œéµä¿®æ­£ï¼šä¸è¦ç”¨ set_index("ç·¨è™Ÿ").to_dict(...) ä»¥å…åˆ— dict å¤±å»ã€Œç·¨è™Ÿã€æ¬„
    sys_idx: Dict[str, Dict[str, Any]] = {}
    for _, row in sys_df.iterrows():
        rid = str(row.get("ç·¨è™Ÿ", "")).strip()
        if rid:
            sys_idx[rid] = row.to_dict()

    rows_out: List[Dict[str, Any]] = []

    # ç¢ºä¿æœ‰ç­‰åƒ¹ç´šæ¬„ä½
    if "_é å¯©ç­‰åƒ¹ç´š_éš±è—" not in pre_df.columns:
        pre_df["_é å¯©ç­‰åƒ¹ç´š_éš±è—"] = pre_df["é å¯©åˆ¤å®š"].apply(normalize_status_equiv)

    for _, prow in pre_df.iterrows():
        pid   = str(prow.get("ç·¨è™Ÿ","")).strip()
        pitem = str(prow.get("æª¢æ ¸é …ç›®",""))
        pori  = str(prow.get("é å¯©åˆ¤å®š",""))             # é¡¯ç¤ºç”¨ï¼šç¬¦åˆ/ä¸é©ç”¨/ç©ºç™½ï¼›A0 = å…­é¸ä¸€å­—é¢
        peq   = str(prow.get("_é å¯©ç­‰åƒ¹ç´š_éš±è—",""))     # æ­£è¦åŒ–ï¼šç¬¦åˆ/ä¸é©ç”¨/æœªæåŠ

        matched = None
        matched_id = ""

        if pid and pid in sys_idx:
            matched = sys_idx[pid]; matched_id = pid
        else:
            # è‹¥ç·¨è™Ÿç©ºç™½æˆ–ä¸åœ¨ç³»çµ±æ¸…å–®ï¼Œå˜—è©¦ä»¥æ–‡å­—ç›¸ä¼¼åº¦
            best_id, best_ratio = fuzzy_match(list(sys_idx.keys()), pid or pitem)
            if best_ratio >= 0.95 and best_id in sys_idx:
                matched = sys_idx[best_id]; matched_id = best_id

        if matched:
            # â˜… A0 ç”¨å­—é¢æ¯”å°ï¼ˆå…­é¸ä¸€ï¼‰ï¼›å…¶é¤˜ç”¨å››æ…‹ï¼ˆç­‰åƒ¹ç´šï¼‰æ¯”å°
            if matched_id == "A0":
                diff = "ä¸€è‡´" if pori.strip() == str(matched.get("ç¬¦åˆæƒ…å½¢","")).strip() else "ä¸ä¸€è‡´"
            else:
                diff = "ä¸€è‡´" if matched.get("ç¬¦åˆæƒ…å½¢","") == peq else "ä¸ä¸€è‡´"

            rows_out.append({
                "é¡åˆ¥": matched.get("é¡åˆ¥",""),
                "ç·¨è™Ÿ": matched_id,
                "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰": matched.get("æª¢æ ¸é …ç›®",""),
                "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": pori,
                "é å¯©ç­‰åƒ¹ç´š": peq,
                "ç³»çµ±æª¢æ ¸çµæœ": matched.get("ç¬¦åˆæƒ…å½¢",""),
                "å·®ç•°åˆ¤å®š": diff,
                "å·®ç•°èªªæ˜/å»ºè­°": matched.get("æ”¹å–„å»ºè­°","") if diff=="ä¸ä¸€è‡´" else "",
                "å°æ‡‰é æ¬¡/å‚™è¨»": prow.get("å°æ‡‰é æ¬¡/å‚™è¨»","")
            })
        else:
            rows_out.append({
                "é¡åˆ¥": "",
                "ç·¨è™Ÿ": pid or "ï¼ˆæœªè­˜åˆ¥ï¼‰",
                "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰": pitem,
                "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": pori,
                "é å¯©ç­‰åƒ¹ç´š": peq or "æœªæåŠ",
                "ç³»çµ±æª¢æ ¸çµæœ": "ï¼ˆç„¡å°æ‡‰ï¼‰",
                "å·®ç•°åˆ¤å®š": "é å¯©å¤šå‡º",
                "å·®ç•°èªªæ˜/å»ºè­°": "æ­¤é å¯©é …ç›®åœ¨ç³»çµ±æª¢æ ¸æ¸…å–®ä¸­ç„¡ç›´æ¥å°æ‡‰ï¼›è«‹äººå·¥ç¢ºèªæ˜¯å¦éœ€ç´å…¥æ¸…å–®æˆ–ç‚ºè¡¨è¿°å·®ç•°ã€‚",
                "å°æ‡‰é æ¬¡/å‚™è¨»": prow.get("å°æ‡‰é æ¬¡/å‚™è¨»","")
            })

    # ç³»çµ±æœ‰ä½†é å¯©æ²’æœ‰ï¼ˆé‡å° A~Fï¼‰
    pre_ids = set([str(x).strip() for x in pre_df.get("ç·¨è™Ÿ", pd.Series(dtype=str)).tolist() if str(x).strip()])
    for _, srow in sys_df.iterrows():
        sid = str(srow.get("ç·¨è™Ÿ","")).strip()
        if sid and sid not in pre_ids:
            rows_out.append({
                "é¡åˆ¥": srow.get("é¡åˆ¥",""),
                "ç·¨è™Ÿ": srow.get("ç·¨è™Ÿ",""),
                "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰": srow.get("æª¢æ ¸é …ç›®",""),
                "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": "",
                "é å¯©ç­‰åƒ¹ç´š": "æœªæåŠ",
                "ç³»çµ±æª¢æ ¸çµæœ": srow.get("ç¬¦åˆæƒ…å½¢",""),
                "å·®ç•°åˆ¤å®š": "ç³»çµ±å¤šå‡º",
                "å·®ç•°èªªæ˜/å»ºè­°": "é å¯©æœªæ¶µè“‹æ­¤ç³»çµ±æª¢æ ¸é …ç›®ï¼Œå»ºè­°è£œåˆ—æˆ–æ–¼æœƒå¯©æ™‚æç¤ºæ‰¿è¾¦æ³¨æ„ã€‚",
                "å°æ‡‰é æ¬¡/å‚™è¨»": ""
            })

    out = pd.DataFrame(rows_out)
    # ä¾ Aâ†’Bâ†’Câ†’Dâ†’Eâ†’F èˆ‡ç·¨è™Ÿæ’åº
    try:
        out["ä¸»ç¢¼"] = out["ç·¨è™Ÿ"].str.extract(r"([A-F])")
        out["å­ç¢¼å€¼"] = pd.to_numeric(out["ç·¨è™Ÿ"].str.extract(r"(\d+(?:\.\d+)?)")[0], errors="coerce")
        code_order = {"A":0, "B":1, "C":2, "D":3, "E":4, "F":5}
        out["ä¸»åº"] = out["ä¸»ç¢¼"].map(code_order).fillna(9)
        out = out.sort_values(["ä¸»åº","å­ç¢¼å€¼","ç·¨è™Ÿ"], kind="mergesort").drop(columns=["ä¸»ç¢¼","å­ç¢¼å€¼","ä¸»åº"])
    except Exception:
        pass
    return out

# ==================== è¡¨æ ¼æ¸²æŸ“ ====================
def render_wrapped_table(df: pd.DataFrame, height_vh: int = 80):
    df2 = df.copy()
    for c in df2.columns:
        df2[c] = df2[c].astype(str).str.replace('\n','<br>')
    style = f"""
    <style>
    .wrap-table-container {{max-height:{height_vh}vh; overflow:auto; border:1px solid #e5e7eb; border-radius:8px;}}
    .wrap-table-container table {{width:100%; border-collapse:collapse; table-layout:fixed; font-size:14px;}}
    .wrap-table-container th, .wrap-table-container td {{border:1px solid #efefef; padding:6px 10px; text-align:left; vertical-align:top; white-space:pre-wrap; word-break:break-word;}}
    .wrap-table-container thead th {{position:sticky; top:0; background:#fafafa; z-index:1;}}
    </style>
    """
    st.markdown(style, unsafe_allow_html=True)
    html = df2.to_html(index=False, escape=False)
    st.markdown(f'<div class="wrap-table-container">{html}</div>', unsafe_allow_html=True)

# ==================== ä¸»ç¨‹å¼ ====================
def main():
    st.set_page_config("ğŸ“‘ æœƒè¾¦è³‡è¨Šè™•æ¡è³¼ç°½å¯©æŸ¥ç³»çµ±(æ¸¬è©¦ç‰ˆ)", layout="wide")
    st.title("ğŸ“‘  æœƒè¾¦è³‡è¨Šè™•æ¡è³¼ç°½å¯©æŸ¥ç³»çµ±(æ¸¬è©¦ç‰ˆ)")

    # RFP/å¥‘ç´„ PDFï¼ˆå¿…å¡«ï¼‰
    uploaded_files = st.file_uploader("ä¸€ã€ğŸ“¥ ä¸Šå‚³ RFP/å¥‘ç´„ PDFï¼ˆå¯è¤‡é¸ï¼‰", type=["pdf"], accept_multiple_files=True)

    # é å…ˆå¯©æŸ¥è¡¨ PDFï¼ˆå¯ç•¥éï¼‰
    pre_files = st.file_uploader("äºŒã€ğŸ“¥ ä¸Šå‚³ã€åŸ·è¡Œå–®ä½é å…ˆå¯©æŸ¥è¡¨ã€PDFï¼ˆå¯è¤‡é¸/å¯ç•¥éï¼‰", 
                                 type=["pdf"], accept_multiple_files=True)

    project_name = st.text_input("ä¸‰ã€æ¡ˆä»¶/å°ˆæ¡ˆåç¨±ï¼ˆå°‡ç”¨æ–¼æª”åï¼‰", value="æœªå‘½åæ¡ˆä»¶")
    mode = st.radio(
        "æª¢æ ¸æ¨¡å¼",
        ("ä¸€æ¬¡æ€§å¯©æŸ¥"),
        horizontal=True,
    )

    if st.button("ğŸš€ é–‹å§‹å¯©æŸ¥", disabled=not uploaded_files):
        
        checklist_all = build_rfp_checklist()

        # é€²åº¦æ¢
        progress_text = st.empty(); progress_bar = st.progress(0)
        def set_progress(p, msg):
            progress_bar.progress(max(0, min(int(p), 100))); progress_text.write(msg)

        # 1) è§£æ RFP/å¥‘ç´„ PDF
        set_progress(5, "ğŸ“„ è§£æèˆ‡å½™æ•´ RFP/å¥‘ç´„ æ–‡ä»¶æ–‡å­—â€¦")
        corpora = []; total_files = len(uploaded_files)
        st.info("ğŸ“„ é–‹å§‹è§£æ RFP/å¥‘ç´„ PDF æª”æ¡ˆâ€¦")

        for i, f in enumerate(uploaded_files):
            set_progress(int((i/max(1,total_files))*30), f"ğŸ“„ è§£æ {f.name} ({i+1}/{total_files})â€¦")
            pdf_bytes = f.read(); text = extract_text_with_headers(pdf_bytes, f.name)
            if not text.strip():
                st.warning(f"âš ï¸ {f.name} çœ‹èµ·ä¾†æ˜¯æƒæå½±åƒ PDFï¼Œç„¡æ³•ç›´æ¥æŠ½æ–‡å­—ã€‚è«‹æä¾›å¯æœå°‹çš„ PDFã€‚")
            corpora.append(text)
        corpus_text = "\n\n".join(corpora)

        # 2) è§£æ é å…ˆå¯©æŸ¥è¡¨ PDFï¼ˆå¯ç•¥éï¼‰
        set_progress(32, "ğŸ§© è™•ç†é å…ˆå¯©æŸ¥è¡¨â€¦")
        pre_df = pd.DataFrame()
        if pre_files:
            pre_texts = []
            for pf in pre_files:
                if is_pdf(pf.name):
                    st.write(f"ğŸ“„ æ­£åœ¨è™•ç†ï¼š{pf.name}")
                    pbytes = pf.read()
                    ptext = extract_text_with_headers(pbytes, pf.name)
                    if ptext.strip():
                        pre_texts.append(ptext)
                    else:
                        st.warning(f"âš ï¸ {pf.name} å¯èƒ½æ˜¯æƒæå½±åƒ PDFï¼Œç„¡æ³•ç›´æ¥æŠ½æ–‡å­—ã€‚è«‹æä¾›å¯æœå°‹ PDFã€‚")
            if pre_texts:
                st.info("ğŸ“„ é–‹å§‹è§£æé å¯©è¡¨ PDF æª”æ¡ˆâ€¦")
                pre_corpus = "\n\n".join(pre_texts)
                prompt = make_precheck_parse_prompt(pre_corpus)
                try:
                    st.info("ğŸ¤– å‘¼å«æ¨¡å‹é€²è¡Œé å¯©è¡¨çµæ§‹åŒ–è¾¨è­˜â€¦")
                    resp = model.generate_content(prompt)
                    st.info("ğŸ“¦ è§£ææ¨¡å‹å›å‚³çš„ JSON çµæ§‹â€¦")
                    rows = parse_precheck_json(resp.text)
                    if rows:
                        st.info("ğŸ“Š å°‡é å¯©è¡¨è½‰ç‚º DataFrame è¡¨æ ¼â€¦")
                        pre_df = precheck_rows_to_df(rows)
                except Exception as e:
                    st.warning(f"âš ï¸ é å¯©è¡¨è§£æå¤±æ•—ï¼š{e}ï¼Œè«‹ç¨å¾Œé‡è©¦æˆ–æ”¹ä¸Šå‚³å¦ä¸€ä»½ PDFã€‚")

            if not pre_df.empty:
              #  st.subheader("ğŸ” é å¯©è¾¨è­˜è¡¨ï¼ˆè«‹å…ˆæª¢è¦–æ˜¯å¦æ­£ç¢ºï¼‰")
                pre_display_cols = ["ç·¨è™Ÿ", "æª¢æ ¸é …ç›®", "é å¯©åˆ¤å®š", "å°æ‡‰é æ¬¡/å‚™è¨»"]
               # render_wrapped_table(pre_df[pre_display_cols], height_vh=40)
            else:
                st.info("â„¹ï¸ æœªä¸Šå‚³æˆ–æœªæˆåŠŸè¾¨è­˜ä»»ä½•é å¯©è¡¨å…§å®¹ã€‚")

        set_progress(35, "ğŸ§  æª¢æ ¸æº–å‚™ä¸­â€¦")

        # 3) ä¾æ¨¡å¼åŸ·è¡Œæª¢æ ¸ï¼ˆä¸€æ¬¡æ€§ï½œæ‰¹æ¬¡ AB/CDEFï½œé€é¡Œï¼‰
        all_results: List[Dict[str, Any]] = []
        st.info(f"ğŸ§ª åŸ·è¡Œç³»çµ±æª¢æ ¸æ¨¡å¼ï¼š{mode}")
        if mode.startswith("ä¸€"):
            groups = group_items_by_ABCDE(checklist_all); st.info("ä¸€æ¬¡æ€§å¯©æŸ¥ä¸­")
        elif mode.startswith("æ‰¹"):
            groups = group_items_by_AB_CDE(checklist_all); st.info("æ‰¹æ¬¡å¯©æŸ¥ä¸­ï¼ˆABï½œCDEFï¼‰")
        else:
            groups = None  # é€é¡Œ

        if groups is not None:
            total_batches = len(groups)
            for bi, (code, items) in enumerate(groups):
                set_progress(35 + int((bi/max(1,total_batches))*55), f"ğŸ” ç¬¬ {bi+1}/{total_batches} æ‰¹ï¼ˆ{code}ï¼‰â€¦ å…± {len(items)} é …")
                prompt = make_batch_prompt(code, items, corpus_text)
                try:
                    resp = model.generate_content(prompt)
                    arr = parse_json_array(resp.text)
                except Exception:
                    arr = []
                allowed_ids = {it['id'] for it in items}
                id_to_meta = {it['id']: it for it in items}
                normalized = []
                for d in arr if isinstance(arr, list) else []:
                    if not isinstance(d, dict): 
                        continue
                    rid = d.get('id')
                    if rid not in allowed_ids:
                        continue
                    meta = id_to_meta[rid]
                    normalized.append({
                        'id': rid,
                        'category': d.get('category', meta['category']),
                        'item': d.get('item', meta['item']),
                        'compliance': d.get('compliance', ''),
                        'evidence': d.get('evidence', []),
                        'recommendation': d.get('recommendation', ''),
                    })
                returned_ids = {x['id'] for x in normalized}
                for it in items:
                    if it['id'] not in returned_ids:
                        normalized.append({
                            'id': it['id'], 'category': it['category'], 'item': it['item'],
                            'compliance': 'æœªæåŠ', 'evidence': [], 'recommendation': ''
                        })
                all_results.extend(normalized)
        else:
            # é€é¡Œæ¨¡å¼
            items_ordered = order_items_AB_C_D_E(checklist_all)
            total_items = len(items_ordered)
            st.info("é€é¡Œæª¢æ ¸ä¸­")
            for i, it in enumerate(items_ordered):
                set_progress(35 + int((i/max(1,total_items))*55), f"ğŸ§© ç¬¬ {i+1}/{total_items} é¡Œï¼š{it['id']} â€¦")
                prompt = make_single_prompt(it, corpus_text)
                try:
                    resp = model.generate_content(prompt)
                    arr = parse_json_array(resp.text)
                except Exception:
                    arr = []
                picked = None
                for d in arr if isinstance(arr, list) else []:
                    if isinstance(d, dict) and d.get('id') == it['id']:
                        picked = d; break
                if picked is None:
                    picked = {
                        'id': it['id'], 'category': it['category'], 'item': it['item'],
                        'compliance': 'æœªæåŠ', 'evidence': [], 'recommendation': ''
                    }
                else:
                    picked.setdefault('category', it['category'])
                    picked.setdefault('item', it['item'])
                    picked.setdefault('compliance', '')
                    picked.setdefault('evidence', [])
                    picked.setdefault('recommendation', '')
                all_results.append(picked)

        # 4) æª¢æ ¸çµæœ â†’ è¡¨æ ¼
        set_progress(92, "ğŸ“¦ å½™æ•´èˆ‡è½‰è¡¨æ ¼â€¦")
        df = to_dataframe(all_results)
        st.success("âœ… å¯©æŸ¥å®Œæˆ")
     #   render_wrapped_table(df, height_vh=52)

        reply_prompt = make_reply_prompt(corpus_text)
        try:
            reply_resp = model.generate_content(reply_prompt)
            reply_text = reply_resp.text.strip()
            st.subheader("ğŸ“Œ å»ºè­°å›å¾©å…§å®¹")
            st.markdown(reply_text)
        except Exception as e:
            st.warning(f"âš ï¸ å»ºè­°å›å¾©ç”Ÿæˆå¤±æ•—ï¼š{e}")



        
        # 5) å·®ç•°å°ç…§ï¼ˆè‹¥æœ‰é å¯©ï¼‰
        cmp_df = pd.DataFrame()
        if not pre_df.empty and not df.empty:
            st.info("ğŸ“‹ å»ºç«‹é å¯©èˆ‡ç³»çµ±æª¢æ ¸çš„å·®ç•°å°ç…§è¡¨â€¦")
            cmp_df = build_compare_table(sys_df=df, pre_df=pre_df)
            st.subheader("ğŸ§¾ å·®ç•°å°ç…§è¡¨ï¼ˆé å¯© vs. ç³»çµ±æª¢æ ¸ï¼‰")
            
            view_df = cmp_df[cmp_df["å·®ç•°åˆ¤å®š"] != "ä¸€è‡´"] 
            
            # åªä¿ç•™æŒ‡å®šæ¬„ä½
            cmp_display_cols = ["é¡åˆ¥", "ç·¨è™Ÿ", "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰", "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰", "å°æ‡‰é æ¬¡/å‚™è¨»", "ç³»çµ±æª¢æ ¸çµæœ", "å·®ç•°èªªæ˜/å»ºè­°"]
            view_df = view_df[cmp_display_cols]


            # å¯åŠ ä¸Šæœå°‹æ¬„ä½ï¼ˆé¸ç”¨ï¼‰
            search_term = st.text_input("ğŸ” æœå°‹æª¢æ ¸é …ç›®")
            if search_term:
                view_df = view_df[view_df["æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰"].str.contains(search_term, case=False, na=False)]

            # é¡¯ç¤ºäº’å‹•è¡¨æ ¼
            st.data_editor(
                view_df,
                use_container_width=True,
                hide_index=True,
                disabled=["é¡åˆ¥", "ç·¨è™Ÿ", "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰", "ç³»çµ±æª¢æ ¸çµæœ"],  # ç¦æ­¢ç·¨è¼¯é€™äº›æ¬„ä½
                column_config={
                    "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": st.column_config.SelectboxColumn(
                        "é å¯©åˆ¤å®š", options=["ç¬¦åˆ", "ä¸é©ç”¨","","æœªæåŠ"], required=False)})
            # åŒ¯å‡º CSV
            csv = view_df.to_csv(index=False).encode("utf-8-sig")
            

           # render_wrapped_table(view_df, height_vh=40)

        # 6) Excel åŒ¯å‡ºï¼ˆ3 å·¥ä½œè¡¨ï¼‰
        try:
            from openpyxl.styles import Alignment
            xbio = io.BytesIO()
            st.info("ğŸ“ åŒ¯å‡º Excelï¼ˆæª¢æ ¸çµæœï¼‹é å¯©è¾¨è­˜ï¼‹å·®ç•°å°ç…§ï¼‰â€¦")
            with pd.ExcelWriter(xbio, engine='openpyxl') as writer:
                # Sheet1: æª¢æ ¸çµæœ
                df.to_excel(writer, index=False, sheet_name='æª¢æ ¸çµæœ')
                ws = writer.sheets['æª¢æ ¸çµæœ']
                for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(wrap_text=True, vertical='top')
                for col_cells in ws.columns:
                    max_len = 12
                    for c in col_cells:
                        val = str(c.value) if c.value is not None else ''
                        max_len = max(max_len, min(80, len(val)))
                    ws.column_dimensions[col_cells[0].column_letter].width = min(60, max_len * 1.2)

                # Sheet2: é å¯©è¾¨è­˜ï¼ˆåªè¼¸å‡ºäº”æ¬„ï¼‰
                if not pre_df.empty:
                    pre_display_cols = ["ç·¨è™Ÿ", "æª¢æ ¸é …ç›®", "é å¯©åˆ¤å®š", "å°æ‡‰é æ¬¡/å‚™è¨»"]
                    pre_df[pre_display_cols].to_excel(writer, index=False, sheet_name='é å¯©è¾¨è­˜')
                    ws2 = writer.sheets['é å¯©è¾¨è­˜']
                    for row in ws2.iter_rows(min_row=1, max_row=ws2.max_row, min_col=1, max_col=ws2.max_column):
                        for cell in row:
                            cell.alignment = Alignment(wrap_text=True, vertical='top')

                # Sheet3: å·®ç•°å°ç…§
                if not cmp_df.empty:
                    cmp_df.to_excel(writer, index=False, sheet_name='å·®ç•°å°ç…§')
                    ws3 = writer.sheets['å·®ç•°å°ç…§']
                    for row in ws3.iter_rows(min_row=1, max_row=ws3.max_row, min_col=1, max_col=ws3.max_column):
                        for cell in row:
                            cell.alignment = Alignment(wrap_text=True, vertical='top')

            xbio.seek(0)
            st.download_button(
                label='ğŸ“¥ ä¸‹è¼‰ Excelï¼ˆæª¢æ ¸ï¼‹é å¯©ï¼‹å°ç…§ï¼‰',
                data=xbio.getvalue(),
                file_name=f"{project_name}_RFP_Contract_Checklist_Compare.xlsx",
                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            )
        except Exception as e:
            st.warning(f"Excel åŒ¯å‡ºå¤±æ•—ï¼š{e}")

        progress_text.empty(); progress_bar.empty()

if __name__ == '__main__':
    main()
