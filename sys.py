# -*- coding: utf-8 -*-
"""
sys.py â€” RFP/å¥‘ç´„ å¯©æŸ¥ï¼ˆè³‡è¨Šè™•æª¢æ ¸ç‰ˆï¼‰ + é å…ˆå¯©æŸ¥è¡¨ï¼ˆPDF å°ˆç”¨ï¼‰
åŠŸèƒ½ï¼š
- ä¸Šå‚³ RFP/å¥‘ç´„ PDFï¼ˆå¯è¤‡é¸ï¼‰â†’ ä¾æª¢æ ¸æ¸…å–®æª¢æ ¸ï¼ˆä¸€æ¬¡æ€§/æ‰¹æ¬¡/é€é¡Œï¼‰
- ä¸Šå‚³ã€ŒåŸ·è¡Œå–®ä½é å…ˆå¯©æŸ¥è¡¨ã€PDFï¼ˆå¯è¤‡é¸/å¯ç•¥éï¼‰â†’ LLM çµæ§‹åŒ–æŠ½å– â†’ å…ˆé¡¯ç¤ºã€é å¯©è¾¨è­˜è¡¨ã€‘
- ç”¢ç”Ÿã€å·®ç•°å°ç…§è¡¨ã€‘ï¼ˆé å¯© vs. ç³»çµ±æª¢æ ¸ï¼‰ï¼Œæ”¯æ´åªé¡¯ç¤ºä¸ä¸€è‡´/ç¼ºæ¼
- åŒ¯å‡º Excelï¼ˆ3 å€‹å·¥ä½œè¡¨ï¼‰ï¼šæª¢æ ¸çµæœ / é å¯©è¾¨è­˜ / å·®ç•°å°ç…§
"""

import os, re, json, io
from typing import List, Dict, Any, Tuple
import streamlit as st
import fitz  # PyMuPDF
import pandas as pd
from dotenv import load_dotenv
import google.generativeai as genai
from difflib import SequenceMatcher

# -------------------- LLM --------------------
load_dotenv()
if os.getenv('GOOGLE_API_KEY'):
    genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
model = genai.GenerativeModel("gemini-2.5-flash")

# -------------------- æª”æ¡ˆå‹æ…‹ --------------------
def is_pdf(name: str) -> bool:
    return name.lower().endswith(".pdf")

# ==================== æª¢æ ¸æ¸…å–® ====================
def build_rfp_checklist() -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    def add(cat, code, text): items.append({"category":cat, "id":code, "item":text})
    # A
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A0", "æœ¬æ¡ˆå±¬é–‹ç™¼å»ºç½®ã€ç³»çµ±ç¶­é‹ã€åŠŸèƒ½å¢ä¿®ã€å¥—è£è»Ÿé«”ã€ç¡¬é«”ã€å…¶ä»–?")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A1", "æœ¬æ¡ˆç‚ºå»¶çºŒæ€§åˆç´„ï¼Œå‰æ¡ˆæ¡è³¼ç°½é™³å½±æœ¬å·²é™„ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.1", "æœ¬æ¡ˆäº‹å‰æ›¾èˆ‡è³‡è¨Šè™•è¨è«–ï¼šæœ¬æ¡ˆç›¸é—œæŠ€è¡“æ–‡ä»¶ç”±è³‡è¨Šè™•å”åŠ©æ’°å¯«ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.2", "æœ¬æ¡ˆäº‹å‰æ›¾èˆ‡è³‡è¨Šè™•è¨è«–ï¼šè¦åŠƒéšæ®µæ›¾èˆ‡è³‡è¨Šè™•é–‹æœƒè¨è«–æ¡è³¼å…§å®¹ä¸¦æœ‰æœƒè­°ç´€éŒ„ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.3", "æœ¬æ¡ˆç°½è¾¦ä¹‹å‰ï¼Œå·²ä»¥è«‹è¾¦å–®éäº¤å¥‘ç´„æ›¸ã€éœ€æ±‚èªªæ˜æ›¸ç­‰ç›¸é—œæ–‡ä»¶ï¼Œé€è«‹è³‡è¨Šè™•æª¢è¦–ï¼Œä¸¦ä¿ç•™è‡³å°‘5å€‹å·¥ä½œå¤©ä¹‹å¯©é–±æœŸå¾Œå–å¾—å›è¦†ã€‚")
    add("A åŸºæœ¬èˆ‡å‰æ¡ˆ", "A2.4", "æœ¬æ¡ˆäº‹å‰æœªèˆ‡è³‡è¨Šè™•è¨è«–ï¼ˆç„¡ï¼‰ã€‚")
    # B
    add("B ç¾æ³èªªæ˜", "B1.1", "æä¾›æœ€æ–°ç‰ˆç¡¬é«”è¨­å‚™åŠç¶²è·¯ä¹‹æ¶æ§‹åœ–(ä¸å«IP Address)ï¼šæ˜ç¢ºè¡¨é”ç¡¬é«”æ”¾ç½®å€åŸŸï¼ˆå«æ©Ÿæˆ¿/å€åŸŸï¼‰ã€‚")
    add("B ç¾æ³èªªæ˜", "B1.2", "æä¾›ç¶²è·¯ä»‹æ¥æ–¹å¼èˆ‡é–‹ç™¼å·¥å…·ä¹‹å» ç‰Œã€å‹è™Ÿã€ç‰ˆæœ¬ç­‰è³‡è¨Šã€‚")
    add("B ç¾æ³èªªæ˜", "B2", "ç½®æ–¼æœ¬éƒ¨æ©Ÿæˆ¿ä¹‹ç³»çµ±ï¼Œå¦‚å¦è¨­å°å¤–é€£ç·šç·šè·¯è€…ï¼Œæä¾›é€£ç·šå°è±¡ã€ç¨®é¡åŠè¦æ ¼æ¸…å–®ã€‚")
    add("B ç¾æ³èªªæ˜", "B3", "æä¾›ä½¿ç”¨è€…æˆ–ä½¿ç”¨æ©Ÿé—œä¹‹ç¤ºæ„åœ–æˆ–èªªæ˜ã€‚")
    add("B ç¾æ³èªªæ˜", "B4", "æä¾›æœ€æ–°ç¶²ç«™ç¶²å€ã€‚")
    add("B ç¾æ³èªªæ˜", "B5", "æä¾›æ‡‰ç”¨ç³»çµ±åŠŸèƒ½æ¸…å–®æˆ–æ¶æ§‹åœ–ï¼ˆå« OSã€DB åç¨±èˆ‡ç‰ˆæœ¬ï¼‰ã€‚")
    # C
    add("C è³‡å®‰éœ€æ±‚", "C1.1", "ç¬¦åˆæœ¬éƒ¨æ¡è³¼å¥‘ç´„è¦ç¯„ä¹‹è³‡è¨Šå®‰å…¨èˆ‡å€‹è³‡ä¿è­·è¦æ±‚ï¼šå·²å¡«åˆ—å®‰å…¨ç­‰ç´šä¸”èˆ‡æœ€æ–°æ ¸å®šç­‰ç´šç›¸ç¬¦ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.2", "è¦æ±‚ç³»çµ±ç¬¦åˆã€Šè³‡é€šå®‰å…¨è²¬ä»»ç­‰ç´šåˆ†ç´šè¾¦æ³•ã€‹ä¹‹ã€è³‡é€šç³»çµ±é˜²è­·åŸºæº–ã€ã€SSDLC å„éšæ®µå®‰å…¨å·¥ä½œï¼›è¦æ±‚å» å•†æäº¤ã€è³‡é€šç³»çµ±é˜²è­·åŸºæº–è‡ªè©•è¡¨ã€ä¸¦å¢åˆ—ç½°å‰‡ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.3", "è¦æ±‚å» å•†ä¹‹æœå‹™æ°´æº–æ»¿è¶³ç³»çµ±æœ€å¤§å¯å®¹å¿ä¸­æ–·æ™‚é–“ï¼ˆRTOï¼‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.4", "éç½®æ–¼æœ¬éƒ¨æ©Ÿæˆ¿ä¹‹æ ¸å¿ƒè³‡é€šç³»çµ±ï¼Œç´å…¥ SOC ç¯„åœã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.5", "å§”å¤–éœ€æ±‚æ¶‰åŠè³‡é€šæŠ€è¡“æœå‹™ï¼ˆå¦‚é›²ç«¯ï¼‰å·²è©•ä¼°åˆæ³•æ€§ã€æŠ€è¡“ç¶­é‹ã€æ³•éµèˆ‡æ¬Šåˆ©ç¾©å‹™æ­¸å±¬ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C1.6", "è¦æ±‚å» å•†ä¸å¾—ä½¿ç”¨æˆ–è¨­è¨ˆä¸ç¬¦å®‰å…¨è¦ç¯„ä¹‹å¸³è™Ÿå¯†ç¢¼ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C2.1", "å·¨é¡/è³‡å®‰æ¡è³¼æˆ–é«˜ç´šå®‰å…¨ç­‰ç´šæ¡ˆä»¶ï¼šæŠ•æ¨™å» å•†å…·å‚™å®‰å…¨è»Ÿé«”é–‹ç™¼èƒ½åŠ›ä¸¦é€šéè³‡å®‰ç®¡ç†ç³»çµ±é©—è­‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C2.2", "å·¨é¡/è³‡å®‰/é«˜ç´šï¼šå°ˆæ¡ˆç®¡ç†äººå“¡è‡³å°‘1äººå…·è³‡è¨Šå®‰å…¨å°ˆæ¥­èªè­‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C2.3", "å·¨é¡/è³‡å®‰/é«˜ç´šï¼šå°ˆæ¡ˆæŠ€è¡“äººå“¡è‡³å°‘1äººå…·ç¶²è·¯å®‰å…¨æŠ€èƒ½ä¹‹è¨“ç·´è­‰æ›¸æˆ–è­‰ç…§ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C3.1", "å…è¨±åˆ†åŒ…è€…ï¼šåˆ†åŒ…å» å•†é ˆæ¯”ç…§æ‰¿åŒ…å» å•†å…±åŒéµå®ˆè³‡å®‰è¦å®šã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C3.2", "å…è¨±åˆ†åŒ…è€…ï¼šæŠ•æ¨™å» å•†æ–¼å»ºè­°æ›¸æ•˜æ˜åˆ†åŒ…å» å•†åŸºæœ¬è³‡æ–™ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C4", "ä¸å¾—æ¡ç”¨å¤§é™¸å» ç‰Œè³‡é€šè¨Šç”¢å“ï¼ˆå¥‘ç´„è‰æ¡ˆç¬¬å…«æ¢(å…­)åŠ(äºŒ äº”)ï¼‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C5", "ç¬¦åˆã€è³‡é€šç³»çµ±ç±Œç²å„éšæ®µè³‡å®‰å¼·åŒ–æªæ–½åŸ·è¡Œæª¢æ ¸è¡¨ã€ï¼ˆé–‹ç™¼é™„è¡¨1/ç¶­é‹é™„è¡¨2ï¼‰ã€‚")
    add("C è³‡å®‰éœ€æ±‚", "C6", "è³‡æ–™åº«ä¸­æ©Ÿæ•è³‡æ–™å·²æ¡ç”¨æˆ–è¦åŠƒé©ç•¶åŠ å¯†æŠ€è¡“ã€‚")
    # Dï¼ˆç¯€éŒ„ï¼‰
    add("D ä½œæ¥­éœ€æ±‚", "D1", "åˆ—å‡ºæ‰€éœ€è»Ÿç¡¬é«”èˆ‡ç¶²è·¯è¨­å‚™æ¸…å–®ï¼Œèªªæ˜ä½¿ç”¨è³‡è¨Šè™•è¨­å‚™/æ—¢æœ‰è¨­å‚™æˆ–å¦è¡Œæ¡è³¼ï¼ˆå„ªå…ˆ VM/å…±åŒä¾›æ‡‰å¥‘ç´„ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D2", "ç³»çµ±é–‹ç™¼æˆ–åŠŸèƒ½å¢ä¿®æ‡‰åˆ—å‡ºæ‰€éœ€ç³»çµ±åŠŸèƒ½ï¼ˆåœ°æ–¹æ”¿åºœç³»çµ±å»ºè­°æä¾›è³‡æ–™ä¸‹è¼‰æˆ–ä»‹æ¥ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D3", "æ•˜æ˜è³‡è¨Šç³»çµ±èˆ‡å…¶ä»–è»Ÿé«”ç³»çµ±ä¹‹ç›¸äº’é—œä¿‚ä¸¦èªªæ˜æ‰€æœ‰åˆ©å®³é—œä¿‚äººã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D4", "æä¾›æ°‘çœ¾ä¸‹è¼‰æª”æ¡ˆè€…ï¼Œå¢åŠ  ODF æ ¼å¼ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D5", "é–‹ç™¼ App å·²é–±è®€ä¸¦éµå¾ªåœ‹ç™¼æœƒç›¸é—œè¦å®šï¼ˆé™„ä»¶2ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D6", "é–‹ç™¼ App ç¬¦åˆé€šå‚³æœƒã€App ç„¡éšœç¤™é–‹ç™¼æŒ‡å¼•ã€ä¸¦å¡«å ±æª¢æ ¸è¡¨ï¼ˆé™„ä»¶3ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D7", "ç¶²ç«™æœå‹™ä¹‹ç³»çµ±ç¬¦åˆåœ‹ç™¼æœƒã€æ”¿åºœç¶²ç«™æœå‹™ç®¡ç†è¦ç¯„ã€ä¸¦å¡«å ±æª¢æ ¸è¡¨ï¼ˆé™„ä»¶4ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D8", "é‡å°æ¥­å‹™æˆ–å€‹äººè³‡æ–™ï¼Œæä¾›å¾ŒçºŒ OpenData æˆ– MyData æœå‹™å»ºè­°ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D9", "ç³»çµ±ç¶­è­·åŒ…å«å®šæœŸåˆ°å ´ã€ç·Šæ€¥åˆ°å ´ã€è«®è©¢æœå‹™ï¼›SLA èˆ‡ç¸¾æ•ˆæŒ‡æ¨™é€£å‹•ä¸¦è¨­è¨ˆæ»¿æ„åº¦èª¿æŸ¥ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D10", "å±¥ç´„æœå‹™éŠœæ¥å¥‘ç´„æœŸé–“ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D11", "é–‹ç™¼åŠæ¸¬è©¦è¨­å‚™èˆ‡ç’°å¢ƒéœ€æ±‚èªªæ˜ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D12", "æ•™è‚²è¨“ç·´åŠå®¢æœæœå‹™ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D13", "ä¿å›ºæœå‹™ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D14", "ç”¢å“æˆæ¬Š (License) ç¬¦åˆéœ€æ±‚ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D15", "ä½œæ¥­éœ€æ±‚å¿…é ˆç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼ˆè©³è¨»5ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D16", "å¦‚æœ‰ GIS / OpenData / MyData ä½œæ¥­éœ€æ±‚ï¼Œç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼ˆè©³è¨»6ï¼‰ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D17", "ä¸Šç·šå‰å®Œæˆéœ€æ±‚è¨ªè«‡ã€éœ€æ±‚ç¢ºèªèˆ‡æ¸¬è©¦ï¼ˆå«æ•ˆèƒ½æ¸¬è©¦ï¼‰ï¼›æäº¤æ¸¬è©¦è¨ˆç•«èˆ‡æ¸¬è©¦å ±å‘Šã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D18", "æ¶‰åŠé†«ç™‚/å¥åº·è³‡æ–™äº¤æ›è€…ï¼Œç´å…¥ FHIR äº¤æ›æ¨™æº–ã€‚")
    add("D ä½œæ¥­éœ€æ±‚", "D19", "åŠŸèƒ½éœ€æ±‚è¨­è¨ˆè€ƒé‡å°å…¥ AI ä»¥ç¯€çœäººåŠ›/é¿å…éŒ¯èª¤èˆ‡æ±ºç­–åˆ†æåŠé¢¨éšªé è­¦ã€‚")
    # E
    add("E ç”¢å“äº¤ä»˜", "E1", "äº¤ä»˜æ™‚ç¨‹åˆç†ï¼Œä¸¦èˆ‡é–‹ç™¼æ–¹å¼ï¼ˆç€‘å¸ƒ/æ•æ·ï¼‰ä¸€è‡´ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E2", "é–‹ç™¼/å¢ä¿®äº¤ä»˜å“å®Œæ•´ï¼ˆå°ˆæ¡ˆè¨ˆç•«ã€éœ€æ±‚/è¨­è¨ˆã€æ¸¬è©¦è¨ˆç•«/å ±å‘Šã€å»ºç½®è¨ˆç•«ã€æ‰‹å†Šã€æ•™è‚²è¨“ç·´ã€ä¿å›ºç´€éŒ„ã€åŸå§‹ç¢¼/åŸ·è¡Œç¢¼ã€æœ€é«˜æ¬Šé™å¸³å¯†ã€è‡ªè©•è¡¨èˆ‡é›»å­æª”ï¼‰ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E3", "ç¶­è­·äº¤ä»˜å“ï¼ˆå°ˆæ¡ˆåŸ·è¡Œè¨ˆç•«ã€ç¶­è­·å·¥ä½œå ±å‘Šã€æœ€æ–°ç‰ˆè¨­è¨ˆ/æ‰‹å†Šã€æœ€æ–°ç‰ˆåŸå§‹ç¢¼/åŸ·è¡Œç¢¼ã€è‡ªè©•è¡¨èˆ‡é›»å­æª”ï¼‰ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E4", "å¿…é ˆç´å…¥ä¹‹åˆ¶å¼æ–‡å¥ï¼ˆè©³è¨»8ï¼‰ï¼šäº¤ä»˜ä¹‹åŸå§‹ç¨‹å¼ç¢¼ã€åŸ·è¡Œç¢¼ï¼Œæœ¬éƒ¨å¾—è¦æ±‚æ‰¿åŒ…å» å•†æ–¼æœ¬éƒ¨æŒ‡å®šä¹‹ç’°å¢ƒé€²è¡Œå†ç”Ÿæ¸¬è©¦ï¼Œä¸¦æ‡‰æä¾›æ‰€ä½¿ç”¨ä¹‹é–‹ç™¼å·¥å…·ï¼Œä»¥é©—è­‰å…¶æ­£ç¢ºæ€§ã€‚")
    add("E ç”¢å“äº¤ä»˜", "E5", "ç¶²è·¯è¨­å‚™è³¼ç½®æ™‚ï¼Œé©—æ”¶ä»¥å½Œå°æ–¹å¼äº¤ä»˜å¸³å¯†ã€è¨­å®šæª”ã€è¦å‰‡åˆ—è¡¨èˆ‡æ¶æ§‹ç­‰ã€‚")
    return items

# ==================== åˆ†ç¾¤/æ’åºå·¥å…· ====================
def group_items_by_ABCDE(items: List[Dict[str, Any]]) -> List[Tuple[str, List[Dict[str, Any]]]]:
    return [("ABCDE", items)] if items else []

def group_items_by_AB_CDE(items: List[Dict[str, Any]]) -> List[Tuple[str, List[Dict[str, Any]]]]:
    ab  = [it for it in items if it['id'] and it['id'][0] in ('A','B')]
    cde = [it for it in items if it['id'] and it['id'][0] in ('C','D','E')]
    groups = []
    if ab: groups.append(('AB', ab))
    if cde: groups.append(('CDE', cde))
    return groups

def group_items_by_AB_C_D_E(items: List[Dict[str, Any]]) -> List[Tuple[str, List[Dict[str, Any]]]]:
    ab = [it for it in items if it['id'] and it['id'][0] in ('A','B')]
    c  = [it for it in items if it['id'] and it['id'][0] == 'C']
    d  = [it for it in items if it['id'] and it['id'][0] == 'D']
    e  = [it for it in items if it['id'] and it['id'][0] == 'E']
    groups = []
    if ab: groups.append(('AB', ab))
    if c: groups.append(('C', c))
    if d: groups.append(('D', d))
    if e: groups.append(('E', e))
    return groups

# é€é¡Œæ’åºï¼ˆABâ†’Câ†’Dâ†’Eï¼‰
def order_items_AB_C_D_E(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    order_map = {'A':0,'B':1,'C':2,'D':3,'E':4}
    return sorted(items, key=lambda it: (order_map.get(it['id'][0], 9), it['id']))

# ==================== PDF è§£æ ====================
def extract_text_with_headers(pdf_bytes: bytes, filename: str) -> str:
    doc = fitz.open(stream=pdf_bytes, filetype='pdf')
    parts = []
    for i, page in enumerate(doc, start=1):
        text = page.get_text('text').strip()
        if not text:
            blocks = page.get_text('blocks')
            text = "\n\n".join([b[4].strip() for b in blocks if b[4].strip()])
        parts.append(f"\n\n===== ã€æª”æ¡ˆ: {filename} é : {i}ã€‘ =====\n" + text)
    return "\n".join(parts)

# ==================== LLM Prompts ====================
def make_batch_prompt(batch_code: str, items: List[Dict[str, Any]], corpus_text: str) -> str:
    checklist_lines = "\n".join([f"{it['id']}ï½œ{it['item']}" for it in items])
    return f"""
ä½ æ˜¯æ”¿åºœæ©Ÿé—œè³‡è¨Šè™•ä¹‹æ¡è³¼/RFP/å¥‘ç´„å¯©æŸ¥å§”å“¡ã€‚è«‹ä¾ä¸‹åˆ—ã€Œæª¢æ ¸æ¢ç›®ï¼ˆ{batch_code} æ‰¹ï¼‰ã€é€æ¢å¯©æŸ¥æ–‡ä»¶å…§å®¹ä¸¦å›å‚³**å”¯ä¸€ JSON é™£åˆ—**ï¼Œé™£åˆ—å…§æ¯å€‹å…ƒç´ å°æ‡‰ä¸€æ¢æ¢ç›®ã€‚
ã€å¯©æŸ¥åŸå‰‡ã€‘
1) åƒ…ä¾æ–‡ä»¶æ˜è¼‰å…§å®¹åˆ¤æ–·ï¼›æœªæåŠå³æ¨™ç¤ºã€ŒæœªæåŠã€ã€‚
2) è‹¥å±¬ä¸é©ç”¨ï¼ˆä¾‹ï¼šæœªå…è¨±åˆ†åŒ…ï¼‰ï¼Œè«‹å›ã€Œä¸é©ç”¨ã€ä¸¦èªªæ˜ä¾æ“šã€‚
3) å‹™å¿…å¼•ç”¨åŸæ–‡çŸ­å¥èˆ‡æª”å/é ç¢¼ä½œç‚º evidenceã€‚
4) ***åš´ç¦è¼¸å‡ºä»»ä½•èˆ‡è¦æ ¼è¯çµ¡äººã€é›»è©±ã€å§“åã€è¯ç¹«æ–¹å¼æœ‰é—œçš„æ–‡å­—ï¼Œå³ä½¿åŸå§‹æ–‡ä»¶å…§æœ‰ã€‚***
ã€è¼¸å‡ºæ ¼å¼ â€” åƒ…èƒ½è¼¸å‡º JSON é™£åˆ—ï¼Œç„¡ä»»ä½•å¤šé¤˜æ–‡å­—/æ¨™è¨˜ã€‘
[
  {{
    "id": "A1",
    "category": "A åŸºæœ¬èˆ‡å‰æ¡ˆ",
    "item": "æ¢ç›®åŸæ–‡ï¼ˆè«‹å®Œæ•´è¤‡è£½ï¼‰",
    "compliance": "è‹¥ id = 'A0'ï¼šåƒ…èƒ½è¼¸å‡ºå…­é¸ä¸€ã€é–‹ç™¼å»ºç½®ï½œç³»çµ±ç¶­é‹ï½œåŠŸèƒ½å¢ä¿®ï½œå¥—è£è»Ÿé«”ï½œç¡¬é«”ï½œå…¶ä»–ã€‘ï¼›è‹¥ id â‰  'A0'ï¼šåƒ…èƒ½è¼¸å‡ºå››é¸ä¸€ã€ç¬¦åˆï½œéƒ¨åˆ†ç¬¦åˆï½œæœªæåŠï½œä¸é©ç”¨ã€‘ï¼›ç¦æ­¢åŒæ™‚è¼¸å‡ºå¤šå€‹æˆ–å…¶ä»–æ–‡å­—",
    "evidence": [{{"file": "æª”å", "page": é ç¢¼, "quote": "é€å­—å¼•è¿°"}}],
    "recommendation": "è‹¥æœªæåŠ/éƒ¨åˆ†ç¬¦åˆï¼Œè«‹çµ¦å…·é«”è£œå¼·æ–¹å‘ï¼›å¦å‰‡ç•™ç©º"
  }}
]
ã€æœ¬æ‰¹æª¢æ ¸æ¸…å–®ï¼ˆidï½œitemï¼‰ã€‘
{checklist_lines}
ã€æ–‡ä»¶å…¨æ–‡ï¼ˆå«æª”å/é ç¢¼æ¨™è¨»ï¼‰ã€‘
{corpus_text}
""".strip()

def make_single_prompt(item: Dict[str, Any], corpus_text: str) -> str:
    return make_batch_prompt(item['id'], [item], corpus_text)

# ï¼ˆæ–°å¢ï¼‰é å¯©è¡¨æŠ½å–ï¼ˆPDF å°ˆç”¨ï¼‰
def make_precheck_parse_prompt(corpus_text: str) -> str:
    return f"""
ä½ æ˜¯æ”¿åºœæ©Ÿé—œè³‡è¨Šè™•ä¹‹æ¡è³¼å¯©æŸ¥åŠ©ç†ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½æˆ–å¤šä»½ã€ŒåŸ·è¡Œå–®ä½é å…ˆå¯©æŸ¥è¡¨ã€çš„ PDF æ–‡å­—ï¼ˆå·²æ¨™è¨»æª”åèˆ‡é ç¢¼ï¼‰ã€‚
è«‹ä½ å°‡æ¯ä¸€åˆ—/æ¯ä¸€æ¢å¯©æŸ¥é …ç›®è½‰ç‚ºçµæ§‹åŒ– JSON é™£åˆ—ï¼Œé€åˆ—ä¸€ç­†ï¼Œæ¬„ä½å¦‚ä¸‹ï¼š
- "id": æ¢ç›®ç·¨è™Ÿï¼Œè‹¥æ˜ç¢ºå‡ºç¾ï¼ˆå¦‚ "A1", "C2.2"ï¼‰ï¼Œè«‹å¡«ï¼›å¦å‰‡ä»¥ç©ºå­—ä¸²ã€‚
- "item": æª¢æ ¸é …ç›®æ–‡å­—ï¼ˆè«‹å®Œæ•´æ“·å–è¦é»ï¼Œå‹¿çœç•¥ï¼‰ã€‚
- "status": é å¯©åˆ¤å®šåŸå­—çœ¼ï¼ˆä¾‹å¦‚ã€Œç¬¦åˆ/éƒ¨åˆ†ç¬¦åˆ/ä¸ç¬¦åˆ/ä¸é©ç”¨/éœ€è£œä»¶/æ”¹å–„ã€ç­‰ï¼‰ã€‚
- "comment": é å¯©è£œå……èªªæ˜æˆ–ç†ç”±ï¼ˆè‹¥ç„¡å‰‡ç©ºå­—ä¸²ï¼‰ã€‚
- "evidence": é™£åˆ—ï¼Œé€é …åŒ…å« {{"file": æª”å, "page": é ç¢¼, "quote": é€å­—å¼•è¿°}}ï¼Œå‹™å¿…æä¾›è‡³å°‘ä¸€ç­†ã€‚

ã€é‡è¦è¦å‰‡ã€‘
1) åƒ…ä¾æ–‡ä»¶æ˜è¼‰å…§å®¹åˆ¤æ–·ï¼›ä¸è¦ç™¼æ˜è³‡æ–™ã€‚
2) è‹¥åŒä¸€åˆ—å«å¤šå€‹åˆ¤å®šï¼Œè«‹ä»¥æœ€ä¸»è¦/æœ€çµ‚çš„åˆ¤å®šç‚º "status"ï¼Œå…¶é¤˜æ”¾å…¥ "comment"ã€‚
3) å‹™å¿…å¼•ç”¨åŸæ–‡çŸ­å¥èˆ‡æª”å/é ç¢¼ä½œç‚º evidenceã€‚
4) ***åš´ç¦è¼¸å‡ºä»»ä½•èˆ‡è¦æ ¼è¯çµ¡äººã€é›»è©±ã€å§“åã€Emailã€è¯ç¹«æ–¹å¼æœ‰é—œçš„æ–‡å­—ï¼Œå³ä½¿åŸå§‹æ–‡ä»¶å…§æœ‰ã€‚***

ã€è¼¸å‡ºæ ¼å¼ â€” åƒ…èƒ½è¼¸å‡º JSON é™£åˆ—ï¼Œç„¡ä»»ä½•å¤šé¤˜æ–‡å­—/æ¨™è¨˜ã€‘
[
  {{
    "id": "A1",
    "item": "â€¦â€¦",
    "status": "ç¬¦åˆ",
    "comment": "â€¦â€¦",
    "evidence": [{{"file": "xxx.pdf", "page": 3, "quote": "â€¦â€¦"}}]
  }}
]

ã€æ–‡ä»¶å…¨æ–‡ï¼ˆå«æª”å/é ç¢¼æ¨™è¨»ï¼‰ã€‘
{corpus_text}
""".strip()

# ==================== è§£æå·¥å…· ====================
def parse_json_array(text: str) -> List[Dict[str, Any]]:
    t = text.strip()
    t = re.sub(r'^\`\`\`(?:json)?', '', t, flags=re.I).strip()
    t = re.sub(r'\`\`\`$', '', t, flags=re.I).strip()
    if t.startswith('{') and t.endswith('}'):
        try:
            d = json.loads(t); return [d]
        except Exception:
            pass
    start = t.find('['); end = t.rfind(']')
    if start != -1 and end != -1 and end > start:
        t = t[start:end+1]
    data = json.loads(t)
    if isinstance(data, dict):
        data = [data]
    return data

# ï¼ˆæ–°å¢ï¼‰é å¯© JSON â†’ DataFrame
def parse_precheck_json(text: str) -> List[Dict[str, Any]]:
    data = parse_json_array(text)
    rows = []
    for r in data if isinstance(data, list) else []:
        if not isinstance(r, dict):
            continue
        ev = []
        for e in r.get("evidence", []):
            if not isinstance(e, dict): 
                continue
            ev.append({
                "file": e.get("file",""),
                "page": e.get("page", None),
                "quote": e.get("quote","")
            })
        rows.append({
            "id": r.get("id","").strip(),
            "item": r.get("item","").strip(),
            "status": r.get("status","").strip(),
            "comment": r.get("comment","").strip(),
            "evidence": ev,
        })
    return rows

def _format_evidence_list(e_list: List[Dict[str, Any]]) -> str:
    lines = []
    for e in e_list:
        file = e.get('file','')
        page = e.get('page', None)
        quote = e.get('quote','')
        tag = f"p.{page}" if page not in (None, "", "n/a") else ""
        lines.append(f"{file} {tag}ï¼š{quote}".strip())
    return "\n".join(lines)

def normalize_status_equiv(s: str) -> str:
    """é å¯©çš„å„ç¨®èªªæ³• â†’ ç­‰åƒ¹ç´šï¼šç¬¦åˆï½œéƒ¨åˆ†ç¬¦åˆï½œæœªæåŠï½œä¸é©ç”¨"""
    if not s:
        return "æœªæåŠ"
    t = re.sub(r"\s+", "", str(s)).lower()
    mapping = {
        "ç¬¦åˆ": "ç¬¦åˆ", "é€šé": "ç¬¦åˆ", "ok": "ç¬¦åˆ", "pass": "ç¬¦åˆ",
        "ä¸ç¬¦åˆ": "éƒ¨åˆ†ç¬¦åˆ", "å¦": "éƒ¨åˆ†ç¬¦åˆ", "æœªé€šé": "éƒ¨åˆ†ç¬¦åˆ",
        "éƒ¨åˆ†ç¬¦åˆ": "éƒ¨åˆ†ç¬¦åˆ", "éœ€è£œä»¶": "éƒ¨åˆ†ç¬¦åˆ", "éœ€æ”¹å–„": "éƒ¨åˆ†ç¬¦åˆ", "æ”¹å–„ä¸­": "éƒ¨åˆ†ç¬¦åˆ",
        "æœªæåŠ": "æœªæåŠ", "ç„¡": "æœªæåŠ", "ç©ºç™½": "æœªæåŠ", "-": "æœªæåŠ",
        "ä¸é©ç”¨": "ä¸é©ç”¨", "na": "ä¸é©ç”¨", "n/a": "ä¸é©ç”¨",
    }
    if t in mapping:
        return mapping[t]
    if any(k in t for k in ["ä¸ç¬¦åˆ", "æœªé€šé", "è£œä»¶", "æ”¹å–„"]):
        return "éƒ¨åˆ†ç¬¦åˆ"
    if any(k in t for k in ["ä¸é©ç”¨", "n/a", "na"]):
        return "ä¸é©ç”¨"
    if any(k in t for k in ["ç¬¦åˆ", "é€šé", "ok", "pass"]):
        return "ç¬¦åˆ"
    return "æœªæåŠ"

def precheck_rows_to_df(rows: List[Dict[str, Any]]) -> pd.DataFrame:
    ev_text = [_format_evidence_list(r.get("evidence", [])) for r in rows]
    df = pd.DataFrame({
        "ç·¨è™Ÿ": [r.get("id","") for r in rows],
        "æª¢æ ¸é …ç›®": [r.get("item","") for r in rows],
        "é å¯©åˆ¤å®š(åŸå­—)": [r.get("status","") for r in rows],
        "é å¯©èªªæ˜": [r.get("comment","") for r in rows],
        "ä¸»è¦è­‰æ“š": ev_text,
    })
    df["é å¯©ç­‰åƒ¹ç´š"] = df["é å¯©åˆ¤å®š(åŸå­—)"].apply(normalize_status_equiv)
    return df

# ==================== å ±è¡¨ï¼ˆç³»çµ±æª¢æ ¸ â†’ DataFrameï¼‰ ====================
def to_dataframe(results: List[Dict[str, Any]]) -> pd.DataFrame:
    rows = []
    for r in results:
        ev_text = "\n".join([f"{e.get('file','')} p.{e.get('page','')}ï¼š{e.get('quote','')}" for e in r.get('evidence', [])])
        rows.append({
            "é¡åˆ¥": r.get("category",""),
            "ç·¨è™Ÿ": r.get("id",""),
            "æª¢æ ¸é …ç›®": r.get("item",""),
            "ç¬¦åˆæƒ…å½¢": r.get("compliance",""),
            "ä¸»è¦è­‰æ“š": ev_text,
            "æ”¹å–„å»ºè­°": r.get("recommendation",""),
        })
    df = pd.DataFrame(rows)
    # å‹å–„æ’åºï¼ˆAâ†’Eã€æ•¸å­—æ¬¡åºï¼‰
    try:
        df["ä¸»ç¢¼"] = df["ç·¨è™Ÿ"].str.extract(r"([A-Z])")
        df["å­ç¢¼å€¼"] = pd.to_numeric(df["ç·¨è™Ÿ"].str.extract(r"(\d+(?:\.\d+)?)")[0], errors='coerce')
        code_order = {"A":0,"B":1,"C":2,"D":3,"E":4}
        df["ä¸»åº"] = df["ä¸»ç¢¼"].map(code_order).fillna(9)
        df = df.sort_values(["ä¸»åº","å­ç¢¼å€¼","ç·¨è™Ÿ"], kind='mergesort').drop(columns=["ä¸»ç¢¼","å­ç¢¼å€¼","ä¸»åº"])
    except Exception:
        pass
    return df

# ==================== é å¯© vs ç³»çµ± æª¢æ ¸ï¼šå·®ç•°å°ç…§ ====================
def fuzzy_match(best_of: List[str], query: str) -> Tuple[str, float]:
    best_id, best_ratio = "", 0.0
    for cand in best_of:
        r = SequenceMatcher(a=query, b=cand).ratio()
        if r > best_ratio:
            best_ratio, best_id = r, cand
    return best_id, best_ratio

def build_compare_table(sys_df: pd.DataFrame, pre_df: pd.DataFrame) -> pd.DataFrame:
    """
    sys_df ä¾†è‡ª to_dataframe(): æ¬„ä½ [é¡åˆ¥, ç·¨è™Ÿ, æª¢æ ¸é …ç›®, ç¬¦åˆæƒ…å½¢, ä¸»è¦è­‰æ“š, æ”¹å–„å»ºè­°]
    pre_df ä¾†è‡ªé å¯©è¾¨è­˜ï¼š      æ¬„ä½ [ç·¨è™Ÿ, æª¢æ ¸é …ç›®, é å¯©åˆ¤å®š(åŸå­—), é å¯©èªªæ˜, ä¸»è¦è­‰æ“š, é å¯©ç­‰åƒ¹ç´š]
    """
    sys_idx = {str(i): r for i, r in sys_df.set_index("ç·¨è™Ÿ").to_dict(orient="index").items()}
    rows = []

    # é å¯©æ¯ä¸€åˆ— â†’ æ‰¾å°æ‡‰ç³»çµ±æ¢ç›®
    for _, prow in pre_df.iterrows():
        pid   = str(prow["ç·¨è™Ÿ"]).strip()
        pitem = str(prow["æª¢æ ¸é …ç›®"])
        peq   = str(prow["é å¯©ç­‰åƒ¹ç´š"])
        pori  = str(prow["é å¯©åˆ¤å®š(åŸå­—)"])

        matched = None
        if pid and pid in sys_idx:
            matched = sys_idx[pid]
        else:
            best_id, best_ratio = fuzzy_match(list(sys_idx.keys()), pid or pitem)
            if best_ratio >= 0.85 and best_id in sys_idx:
                matched = sys_idx[best_id]

        if matched:
            diff = "ä¸€è‡´" if matched["ç¬¦åˆæƒ…å½¢"] == peq else "ä¸ä¸€è‡´"
            rows.append({
                "é¡åˆ¥": matched["é¡åˆ¥"],
                "ç·¨è™Ÿ": matched["ç·¨è™Ÿ"],
                "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰": matched["æª¢æ ¸é …ç›®"],
                "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": pori,
                "é å¯©ç­‰åƒ¹ç´š": peq,
                "ç³»çµ±æª¢æ ¸çµæœ": matched["ç¬¦åˆæƒ…å½¢"],
                "å·®ç•°åˆ¤å®š": diff,
                "å·®ç•°èªªæ˜/å»ºè­°": matched.get("æ”¹å–„å»ºè­°","") if diff=="ä¸ä¸€è‡´" else "",
            })
        else:
            rows.append({
                "é¡åˆ¥": "",
                "ç·¨è™Ÿ": pid or "ï¼ˆæœªè­˜åˆ¥ï¼‰",
                "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰": pitem,
                "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": pori,
                "é å¯©ç­‰åƒ¹ç´š": peq,
                "ç³»çµ±æª¢æ ¸çµæœ": "ï¼ˆç„¡å°æ‡‰ï¼‰",
                "å·®ç•°åˆ¤å®š": "é å¯©å¤šå‡º",
                "å·®ç•°èªªæ˜/å»ºè­°": "é å¯©åˆ—å‡ºä¹‹é …ç›®åœ¨ç³»çµ±æª¢æ ¸æ¸…å–®ä¸­ç„¡ç›´æ¥å°æ‡‰ï¼Œè«‹äººå·¥ç¢ºèªæ˜¯å¦ç‚ºå®¢è£½æˆ–è¡¨è¿°å·®ç•°ã€‚",
            })

    # åæŸ¥ï¼šç³»çµ±æœ‰ä½†é å¯©æ²’æœ‰
    pre_ids = set([str(x).strip() for x in pre_df["ç·¨è™Ÿ"].tolist() if str(x).strip()])
    for _, srow in sys_df.iterrows():
        sid = str(srow["ç·¨è™Ÿ"]).strip()
        if sid and sid not in pre_ids:
            rows.append({
                "é¡åˆ¥": srow["é¡åˆ¥"],
                "ç·¨è™Ÿ": srow["ç·¨è™Ÿ"],
                "æª¢æ ¸é …ç›®ï¼ˆç³»çµ±åŸºæº–ï¼‰": srow["æª¢æ ¸é …ç›®"],
                "é å¯©åˆ¤å®šï¼ˆåŸå­—ï¼‰": "",
                "é å¯©ç­‰åƒ¹ç´š": "æœªæåŠ",
                "ç³»çµ±æª¢æ ¸çµæœ": srow["ç¬¦åˆæƒ…å½¢"],
                "å·®ç•°åˆ¤å®š": "ç³»çµ±å¤šå‡º",
                "å·®ç•°èªªæ˜/å»ºè­°": "é å¯©æœªæ¶µè“‹æ­¤ç³»çµ±æª¢æ ¸é …ç›®ï¼Œå»ºè­°è£œåˆ—æˆ–æ–¼æœƒå¯©æ™‚æç¤ºæ‰¿è¾¦æ³¨æ„ã€‚",
            })

    out = pd.DataFrame(rows)
    # ä¾ ABâ†’Câ†’Dâ†’E èˆ‡ç·¨è™Ÿæ’åº
    try:
        out["ä¸»ç¢¼"] = out["ç·¨è™Ÿ"].str.extract(r"([A-Z])")
        out["å­ç¢¼å€¼"] = pd.to_numeric(out["ç·¨è™Ÿ"].str.extract(r"(\d+(?:\.\d+)?)")[0], errors="coerce")
        code_order = {"A":0,"B":1,"C":2,"D":3,"E":4}
        out["ä¸»åº"] = out["ä¸»ç¢¼"].map(code_order).fillna(9)
        out = out.sort_values(["ä¸»åº","å­ç¢¼å€¼","ç·¨è™Ÿ"], kind="mergesort").drop(columns=["ä¸»ç¢¼","å­ç¢¼å€¼","ä¸»åº"])
    except Exception:
        pass
    return out

# ==================== è¡¨æ ¼æ¸²æŸ“ ====================
def render_wrapped_table(df: pd.DataFrame, height_vh: int = 80):
    df2 = df.copy()
    for c in df2.columns:
        df2[c] = df2[c].astype(str).str.replace('\n','<br>')
    style = f"""
    <style>
    .wrap-table-container {{max-height:{height_vh}vh; overflow:auto; border:1px solid #e5e7eb; border-radius:8px;}}
    .wrap-table-container table {{width:100%; border-collapse:collapse; table-layout:fixed; font-size:14px;}}
    .wrap-table-container th, .wrap-table-container td {{border:1px solid #efefef; padding:6px 10px; text-align:left; vertical-align:top; white-space:pre-wrap; word-break:break-word;}}
    .wrap-table-container thead th {{position:sticky; top:0; background:#fafafa; z-index:1;}}
    </style>
    """
    st.markdown(style, unsafe_allow_html=True)
    html = df2.to_html(index=False, escape=False)
    st.markdown(f'<div class="wrap-table-container">{html}</div>', unsafe_allow_html=True)

# ==================== ä¸»ç¨‹å¼ ====================
def main():
    st.set_page_config("ğŸ“‘ RFP/å¥‘ç´„å¯©æŸ¥ç³»çµ±(æ¸¬è©¦ç‰ˆ)", layout="wide")
    st.title("ğŸ“‘ è³‡è¨Šæœå‹™æ¡è³¼ RFP/å¥‘ç´„å¯©æŸ¥ç³»çµ±(æ¸¬è©¦ç‰ˆ)")

    # RFP/å¥‘ç´„ PDFï¼ˆå¿…å¡«ï¼‰
    uploaded_files = st.file_uploader("ğŸ“¥ ä¸Šå‚³ RFP/å¥‘ç´„ PDFï¼ˆå¯è¤‡é¸ï¼‰", type=["pdf"], accept_multiple_files=True)

    # é å…ˆå¯©æŸ¥è¡¨ PDFï¼ˆå¯ç•¥éï¼‰
    pre_files = st.file_uploader("ğŸ“¥ ä¸Šå‚³ã€åŸ·è¡Œå–®ä½é å…ˆå¯©æŸ¥è¡¨ã€PDFï¼ˆå¯è¤‡é¸/å¯ç•¥éï¼‰", 
                                 type=["pdf"], accept_multiple_files=True)

    project_name = st.text_input("æ¡ˆä»¶/å°ˆæ¡ˆåç¨±ï¼ˆå°‡ç”¨æ–¼æª”åï¼‰", value="æœªå‘½åæ¡ˆä»¶")
    mode = st.radio(
        "æª¢æ ¸æ¨¡å¼",
        ("ä¸€æ¬¡æ€§å¯©æŸ¥", "æ‰¹æ¬¡å¯©æŸ¥", "é€é¡Œå¯©æŸ¥"),
        horizontal=True,
    )

    if st.button("ğŸš€ é–‹å§‹å¯©æŸ¥", disabled=not uploaded_files):
        checklist_all = build_rfp_checklist()

        # é€²åº¦æ¢
        progress_text = st.empty(); progress_bar = st.progress(0)
        def set_progress(p, msg):
            progress_bar.progress(max(0, min(int(p), 100))); progress_text.write(msg)

        # 1) è§£æ RFP/å¥‘ç´„ PDF
        set_progress(5, "ğŸ“„ è§£æèˆ‡å½™æ•´ RFP/å¥‘ç´„ æ–‡ä»¶æ–‡å­—â€¦")
        corpora = []; total_files = len(uploaded_files)
        for i, f in enumerate(uploaded_files):
            set_progress(int((i/max(1,total_files))*30), f"ğŸ“„ è§£æ {f.name} ({i+1}/{total_files})â€¦")
            pdf_bytes = f.read(); text = extract_text_with_headers(pdf_bytes, f.name)
            if not text.strip():
                st.warning(f"âš ï¸ {f.name} çœ‹èµ·ä¾†æ˜¯æƒæå½±åƒ PDFï¼Œç„¡æ³•ç›´æ¥æŠ½æ–‡å­—ã€‚è«‹æä¾›å¯æœå°‹çš„ PDFã€‚")
            corpora.append(text)
        corpus_text = "\n\n".join(corpora)
        set_progress(32, "ğŸ§© è™•ç†é å…ˆå¯©æŸ¥è¡¨â€¦")

        # 2) è§£æ é å…ˆå¯©æŸ¥è¡¨ PDFï¼ˆå¯ç•¥éï¼‰
        pre_df = pd.DataFrame()
        if pre_files:
            pre_texts = []
            for pf in pre_files:
                if is_pdf(pf.name):
                    pbytes = pf.read()
                    ptext = extract_text_with_headers(pbytes, pf.name)
                    if ptext.strip():
                        pre_texts.append(ptext)
                    else:
                        st.warning(f"âš ï¸ {pf.name} å¯èƒ½æ˜¯æƒæå½±åƒ PDFï¼Œç„¡æ³•ç›´æ¥æŠ½æ–‡å­—ã€‚è«‹æä¾›å¯æœå°‹ PDFã€‚")
            if pre_texts:
                pre_corpus = "\n\n".join(pre_texts)
                prompt = make_precheck_parse_prompt(pre_corpus)
                try:
                    resp = model.generate_content(prompt)
                    rows = parse_precheck_json(resp.text)
                    if rows:
                        pre_df = precheck_rows_to_df(rows)
                except Exception:
                    st.warning("âš ï¸ é å¯©è¡¨è§£æå¤±æ•—ï¼Œè«‹ç¨å¾Œé‡è©¦æˆ–æ”¹ä¸Šå‚³å¦ä¸€ä»½ PDFã€‚")

            if not pre_df.empty:
                st.subheader("ğŸ” é å¯©è¾¨è­˜è¡¨ï¼ˆè«‹å…ˆæª¢è¦–æ˜¯å¦æ­£ç¢ºï¼‰")
                render_wrapped_table(pre_df, height_vh=40)
            else:
                st.info("â„¹ï¸ æœªä¸Šå‚³æˆ–æœªæˆåŠŸè¾¨è­˜ä»»ä½•é å¯©è¡¨å…§å®¹ã€‚")

        set_progress(35, "ğŸ§  æª¢æ ¸æº–å‚™ä¸­â€¦")

        # 3) ä¾æ¨¡å¼åŸ·è¡Œæª¢æ ¸ï¼ˆæ²¿ç”¨åŸæœ¬é‚è¼¯ï¼‰
        all_results: List[Dict[str, Any]] = []
        if mode.startswith("ä¸€"):
            groups = group_items_by_ABCDE(checklist_all); st.info("ä¸€æ¬¡æ€§å¯©æŸ¥ä¸­")
        elif mode.startswith("æ‰¹"):
            groups = group_items_by_AB_CDE(checklist_all); st.info("æ‰¹æ¬¡å¯©æŸ¥ä¸­")
        else:
            groups = None  # é€é¡Œ

        if groups is not None:
            total_batches = len(groups)
            for bi, (code, items) in enumerate(groups):
                set_progress(35 + int((bi/max(1,total_batches))*55), f"ğŸ” ç¬¬ {bi+1}/{total_batches} æ‰¹ï¼ˆ{code}ï¼‰â€¦ å…± {len(items)} é …")
                prompt = make_batch_prompt(code, items, corpus_text)
                try:
                    resp = model.generate_content(prompt)
                    arr = parse_json_array(resp.text)
                except Exception:
                    arr = []
                allowed_ids = {it['id'] for it in items}
                id_to_meta = {it['id']: it for it in items}
                normalized = []
                for d in arr if isinstance(arr, list) else []:
                    if not isinstance(d, dict): 
                        continue
                    rid = d.get('id')
                    if rid not in allowed_ids:
                        continue
                    meta = id_to_meta[rid]
                    normalized.append({
                        'id': rid,
                        'category': d.get('category', meta['category']),
                        'item': d.get('item', meta['item']),
                        'compliance': d.get('compliance', ''),
                        'evidence': d.get('evidence', []),
                        'recommendation': d.get('recommendation', ''),
                    })
                returned_ids = {x['id'] for x in normalized}
                for it in items:
                    if it['id'] not in returned_ids:
                        normalized.append({
                            'id': it['id'], 'category': it['category'], 'item': it['item'],
                            'compliance': 'æœªæåŠ', 'evidence': [], 'recommendation': ''
                        })
                all_results.extend(normalized)
        else:
            # é€é¡Œæ¨¡å¼
            items_ordered = order_items_AB_C_D_E(checklist_all)
            total_items = len(items_ordered)
            st.info("é€é¡Œæª¢æ ¸ä¸­")
            for i, it in enumerate(items_ordered):
                set_progress(35 + int((i/max(1,total_items))*55), f"ğŸ§© ç¬¬ {i+1}/{total_items} é¡Œï¼š{it['id']} â€¦")
                prompt = make_single_prompt(it, corpus_text)
                try:
                    resp = model.generate_content(prompt)
                    arr = parse_json_array(resp.text)
                except Exception:
                    arr = []
                picked = None
                for d in arr if isinstance(arr, list) else []:
                    if isinstance(d, dict) and d.get('id') == it['id']:
                        picked = d; break
                if picked is None:
                    picked = {
                        'id': it['id'], 'category': it['category'], 'item': it['item'],
                        'compliance': 'æœªæåŠ', 'evidence': [], 'recommendation': ''
                    }
                else:
                    picked.setdefault('category', it['category'])
                    picked.setdefault('item', it['item'])
                    picked.setdefault('compliance', '')
                    picked.setdefault('evidence', [])
                    picked.setdefault('recommendation', '')
                all_results.append(picked)

        # 4) æª¢æ ¸çµæœ â†’ è¡¨æ ¼
        set_progress(92, "ğŸ“¦ å½™æ•´èˆ‡è½‰è¡¨æ ¼â€¦")
        df = to_dataframe(all_results)
        st.success("âœ… å¯©æŸ¥å®Œæˆ")
        render_wrapped_table(df, height_vh=52)

        # 5) å·®ç•°å°ç…§ï¼ˆè‹¥æœ‰é å¯©ï¼‰
        cmp_df = pd.DataFrame()
        if not pre_df.empty and not df.empty:
            cmp_df = build_compare_table(sys_df=df, pre_df=pre_df)
            st.subheader("ğŸ§¾ å·®ç•°å°ç…§è¡¨ï¼ˆé å¯© vs. ç³»çµ±æª¢æ ¸ï¼‰")
            show_only_diff = st.checkbox("åªé¡¯ç¤ºã€ä¸ä¸€è‡´/ç¼ºæ¼ã€", value=True)
            view_df = cmp_df[cmp_df["å·®ç•°åˆ¤å®š"] != "ä¸€è‡´"] if show_only_diff else cmp_df
            render_wrapped_table(view_df, height_vh=40)

        # 6) Excel åŒ¯å‡ºï¼ˆ3 å·¥ä½œè¡¨ï¼‰
        try:
            from openpyxl.styles import Alignment
            xbio = io.BytesIO()
            with pd.ExcelWriter(xbio, engine='openpyxl') as writer:
                # Sheet1: æª¢æ ¸çµæœ
                df.to_excel(writer, index=False, sheet_name='æª¢æ ¸çµæœ')
                ws = writer.sheets['æª¢æ ¸çµæœ']
                for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(wrap_text=True, vertical='top')
                for col_cells in ws.columns:
                    max_len = 12
                    for c in col_cells:
                        val = str(c.value) if c.value is not None else ''
                        max_len = max(max_len, min(80, len(val)))
                    ws.column_dimensions[col_cells[0].column_letter].width = min(60, max_len * 1.2)

                # Sheet2: é å¯©è¾¨è­˜
                if not pre_df.empty:
                    pre_df.to_excel(writer, index=False, sheet_name='é å¯©è¾¨è­˜')
                    ws2 = writer.sheets['é å¯©è¾¨è­˜']
                    for row in ws2.iter_rows(min_row=1, max_row=ws2.max_row, min_col=1, max_col=ws2.max_column):
                        for cell in row:
                            cell.alignment = Alignment(wrap_text=True, vertical='top')

                # Sheet3: å·®ç•°å°ç…§
                if not cmp_df.empty:
                    cmp_df.to_excel(writer, index=False, sheet_name='å·®ç•°å°ç…§')
                    ws3 = writer.sheets['å·®ç•°å°ç…§']
                    for row in ws3.iter_rows(min_row=1, max_row=ws3.max_row, min_col=1, max_col=ws3.max_column):
                        for cell in row:
                            cell.alignment = Alignment(wrap_text=True, vertical='top')

            xbio.seek(0)
            st.download_button(
                label='ğŸ“¥ ä¸‹è¼‰ Excelï¼ˆæª¢æ ¸ï¼‹é å¯©ï¼‹å°ç…§ï¼‰',
                data=xbio.getvalue(),
                file_name=f"{project_name}_RFP_Contract_Checklist_Compare.xlsx",
                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            )
        except Exception as e:
            st.warning(f"Excel åŒ¯å‡ºå¤±æ•—ï¼š{e}")

        progress_text.empty(); progress_bar.empty()

if __name__ == '__main__':
    main()
